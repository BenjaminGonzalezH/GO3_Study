{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56853acc",
   "metadata": {},
   "source": [
    "# Archivo a archivo\n",
    "Este archivo tiene el propósito de proporcionar el estudio de cada archivo existente dentro del código Rust del paquete de go3 para explicar su funcionamiento clave, asimismo, se toma la iniciativa de presentar dos razones fundamentales para determinar que una comparación con PyGoSemSim no se considera viable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5bfca",
   "metadata": {},
   "source": [
    "## Código 1 - libs.rs\n",
    "Este es el archivo intermedio que permite la comunicación entre Python y Rust, por ende, ya se deduce que la implementación de las funcionalidades de este paquete son completamente hechas en Rust que es envuelto por el paquete pyo3 que permite invocarlas bajo la sintaxis del lenguaje de destino. A continuación el desglose linea a linea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142486f",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// En esta sección se realiza la envoltura de los archivos Rust con pyo3 estableciendolo en\n",
    "// 3 pasos:\n",
    "use pyo3::prelude::*;           //Rust comprende Python (viceversa).\n",
    "use pyo3::types::PyModule;      //Rust entiende el concepto de módulo de Python.\n",
    "use pyo3::wrap_pyfunction;      //Python invoca código de Rust.\n",
    "\n",
    "// Se solicita la carga de tres archivo existentes en el mismo directorio en el que se\n",
    "// encuentra el archivo actual.\n",
    "pub mod go_loader;              // Lectura de archivos de entrada, relaciones e IC.\n",
    "pub mod go_ontology;            // --\n",
    "pub mod go_semantic;            // --\n",
    "\n",
    "// Dentro de cada uno de estos archivos mencionados previamente, se solicita la disponibilidad\n",
    "// de las funciones que se mencionan a continuación.\n",
    "use go_loader::{load_go_terms, load_gaf, build_term_counter};\n",
    "use go_ontology::{get_term_by_id, ancestors, common_ancestor, deepest_common_ancestor};\n",
    "use go_semantic::{term_ic, semantic_similarity, batch_similarity, compare_genes, compare_gene_pairs_batch, set_num_threads};\n",
    "\n",
    "// En esta sección se realiza la envoltura de todas las componentes invocadas anteriormente\n",
    "// de forma en que estas sean reconocidad como modulo de Python.\n",
    "#[pymodule]\n",
    "fn go3(_py: Python, m: &Bound<'_, PyModule>) -> PyResult<()> {\n",
    "\n",
    "    // Funciones añadidas por archivos.\n",
    "    m.add_function(wrap_pyfunction!(load_go_terms, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(load_gaf, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(build_term_counter, m)?)?;\n",
    "\n",
    "    m.add_function(wrap_pyfunction!(get_term_by_id, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(ancestors, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(common_ancestor, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(deepest_common_ancestor, m)?)?;\n",
    "\n",
    "    m.add_function(wrap_pyfunction!(set_num_threads, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(term_ic, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(semantic_similarity, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(batch_similarity, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(compare_genes, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(compare_gene_pairs_batch, m)?)?;\n",
    "\n",
    "    // Objetos o clases por archivo.\n",
    "    m.add_class::<go_ontology::PyGOTerm>()?;\n",
    "    m.add_class::<go_loader::GAFAnnotation>()?;\n",
    "    m.add_class::<go_loader::TermCounter>()?;\n",
    "\n",
    "    // Retroalimentación que indica exito o error en la migración de funciones.\n",
    "    Ok(())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccaf8f",
   "metadata": {},
   "source": [
    "## Código 2 - goloader.rs\n",
    "Archivo encargado del procesamiento de los archivos .obo y gaf para la construcción de las relaciones jerarquicas entre terminos, relaciones gen a termino y el cálculo de IC de cada término existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc783740",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// ---------------- Librerias.\n",
    "use pyo3::prelude::*;                                               // Rust comprende Python (viceversa).\n",
    "use once_cell::sync::OnceCell;                                      // Implementación de variables globales de una sola carga.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};       // Invocación de tablas hash.\n",
    "use std::io::{BufReader, BufRead};                                  // Lectura eficiente de archivos.\n",
    "use std::fs::File;                                                  // Apertura y manejo de archivos.\n",
    "use parking_lot::RwLock;                                            // Administración de hilos por semaforo.\n",
    "use std::path::Path;                                                // Administración de rutas.\n",
    "use std::fs;                                                        // Administración de entradas y salidas.\n",
    "use reqwest::blocking::get;                                         // Peticiones para obtención de recursos en internet.\n",
    "use rayon::prelude::*;                                              // Automatización del uso de todos los nucleos disponibles de un dispositivo.\n",
    "\n",
    "// ---------------- Componentes globales.\n",
    "// Declaración de entidades estáticas en memoria y global en toda la ejecución del código;\n",
    "// cabe señalar que estos son persistentes mientras el código se ejecuta y estos poseen la\n",
    "// capacidad de ser leidos por varios threads con bloqueo si se requiere escribir.\n",
    "use crate::go_ontology::{GOTerm, PyGOTerm, collect_ancestors, get_terms_or_error};\n",
    "pub static GO_TERMS_CACHE: OnceCell<RwLock<HashMap<String, GOTerm>>> = OnceCell::new();                 // Hash entre el GO_ID y su información.\n",
    "pub static GENE2GO_CACHE: OnceCell<RwLock<HashMap<String, Vec<String>>>> = OnceCell::new();             // Hash entre un gen y sus términos GO relacionados.\n",
    "pub static ANCESTORS_CACHE: OnceCell<RwLock<HashMap<String, HashSet<String>>>> = OnceCell::new();       // Hash entre un GO_ID y sus respectivos ancestros.\n",
    "pub static DCA_CACHE: OnceCell<RwLock<HashMap<(String, String), String>>> = OnceCell::new();            // DCA - Deep Common Ancestor entre un par de terminos.\n",
    "\n",
    "/// Struct representing a single annotation from a GAF file.\n",
    "/// \n",
    "/// Fields\n",
    "/// ------\n",
    "/// db_object_id : str\n",
    "///   The gene product identifier (e.g., UniProt ID).\n",
    "/// go_term : str\n",
    "///   The GO term ID (e.g., GO:0008150).\n",
    "/// evidence : str\n",
    "///   The evidence code for the annotation (e.g., IEA).\n",
    "/*De este componente se puede comprender que solamente del GAF se obtiene el db_object_id\n",
    "el go_term y la evidencia asociada a este mismo, siendo las tres columnas claves.*/\n",
    "#[pyclass]                      // Se considera una clase compatible con Python.\n",
    "#[derive(Clone)]                // Permite replicación/copia si es requerido.\n",
    "pub struct GAFAnnotation {\n",
    "    #[pyo3(get)]\n",
    "    pub db_object_id: String,\n",
    "    #[pyo3(get)]\n",
    "    pub go_term: String,\n",
    "    #[pyo3(get)]\n",
    "    pub evidence: String,\n",
    "}\n",
    "\n",
    "/// Struct holding annotation counts and information content (IC) for GO terms.\n",
    "///\n",
    "/// Fields\n",
    "/// ------\n",
    "/// counts : dict\n",
    "///   Mapping from GO term ID to annotation count.\n",
    "/// total_by_ns : dict\n",
    "///   Mapping from namespace to total annotation count.\n",
    "/// ic : dict\n",
    "///   Mapping from GO term ID to information content (IC).\n",
    "/*Estructura encargada de establecer la computación de information content (IC) de\n",
    "cada termino GO asociado.*/\n",
    "#[pyclass]\n",
    "#[derive(Clone)]\n",
    "pub struct TermCounter {\n",
    "    #[pyo3(get)]\n",
    "    pub counts: HashMap<String, usize>,         // term_id -> count\n",
    "    #[pyo3(get)]\n",
    "    pub total_by_ns: HashMap<String, usize>,    // namespace -> total annotations\n",
    "    #[pyo3(get)]\n",
    "    pub ic: HashMap<String, f64>,               // term_id -> IC\n",
    "}\n",
    "\n",
    "/// Parse a GO OBO file and return a map of GO term IDs to GOTerm structs.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// path : str\n",
    "///   Path to the OBO file.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// dict\n",
    "///   Map of GO term IDs to term structs.\n",
    "pub fn parse_obo(path: &str) -> HashMap<String, GOTerm> {\n",
    "    let contents = fs::read_to_string(path).expect(\"Can't open OBO file\");                          // Carga completa en memoria del archivo\n",
    "    let chunks = contents.split(\"[Term]\");                                                          // Separación de chunks de líneas según campo [Term]\n",
    "\n",
    "    let canonical_terms: Vec<GOTerm> = chunks                                                       // Procesamiento paralelo de GoTerms (ignora obsoletos o mal formateados).\n",
    "        .par_bridge()\n",
    "        .filter_map(parse_term_chunk)\n",
    "        .filter(|term| !term.is_obsolete)\n",
    "        .collect();\n",
    "\n",
    "    let mut term_map: HashMap<String, GOTerm> =                                                     // Preparación de memoria y configurar uso de hashing rápido.\n",
    "        HashMap::with_capacity_and_hasher(canonical_terms.len() * 2, Default::default());\n",
    "\n",
    "    for term in canonical_terms.into_iter() {                                                       // Uso de procesamiento de otros identificadores y términos asociados a un\n",
    "        // Collect the \"synonym group\": canonical + alts                                            // GoTerm de forma en que todos apunten a la misma información.\n",
    "        let mut all_ids = term.alt_ids.clone();\n",
    "        all_ids.push(term.id.clone());\n",
    "\n",
    "        // Build GOTerm copies for each ID\n",
    "        for id in &all_ids {\n",
    "            let mut clone = term.clone();\n",
    "            clone.id = id.clone();\n",
    "            clone.alt_ids = all_ids.clone(); // all point to the same group\n",
    "            term_map.insert(id.clone(), clone);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Compute children, levels, depths, etc.\n",
    "    compute_levels_and_depths(&mut term_map);                                                       // Se verifica profundidad/especifidad de cada término.\n",
    "\n",
    "    // Precompute ancestors for caching\n",
    "    let ancestors_map: HashMap<String, HashSet<String>> = term_map                                  // Se identifican (una sola vez) los ancestros a cada GoTerm.\n",
    "        .par_iter()\n",
    "        .map(|(id, _)| {\n",
    "            let ancestors = crate::go_ontology::collect_ancestors(id, &term_map)\n",
    "                .into_iter()\n",
    "                .map(|s| s.to_string())\n",
    "                .collect();\n",
    "            (id.clone(), ancestors)\n",
    "        })\n",
    "        .collect();\n",
    "    let _ = ANCESTORS_CACHE.set(RwLock::new(ancestors_map));                                        // Guardado en cache.\n",
    "\n",
    "    // Initialize DCA_CACHE\n",
    "    let _ = DCA_CACHE.set(RwLock::new(HashMap::default()));                                         // Se inicializa cache de ancestro más cercano (se calcula por demanda).\n",
    "\n",
    "    term_map                                                                                        // Se retorna mapeo de GoTermns.\n",
    "}\n",
    "\n",
    "/// Parse a chunk of text representing a single GO term from an OBO file.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// chunk : str\n",
    "///   Text chunk for a single term.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// Option[GOTerm]\n",
    "///   Parsed term or None if invalid/obsolete.\n",
    "fn parse_term_chunk(chunk: &str) -> Option<GOTerm> {\n",
    "    // Definición de la estructura de un término (dentro del archivo).\n",
    "    let mut term = GOTerm {\n",
    "        id: String::new(),                              // Identificador.\n",
    "        name: String::new(),                            // Nombre.\n",
    "        namespace: String::new(),                       // Ontología BP, MC, CC.\n",
    "        definition: String::new(),                      // Definición.\n",
    "        parents: Vec::new(),                            // Padres.\n",
    "        is_obsolete: false,                             // ¿Es obsoleto?\n",
    "        alt_ids: Vec::new(),                            // Ids alternativos.\n",
    "        replaced_by: None,                              // Reemplazado por... (si esta obsoleto).\n",
    "        consider: Vec::new(),                           // Considerar ¿Qué cosa?.\n",
    "        synonyms: Vec::new(),                           // Sinónimos.\n",
    "        xrefs: Vec::new(),                              // Referencias asociadas a...\n",
    "        relationships: Vec::new(),                      // Relaciones con otros términois.\n",
    "        comment: None,                                  // Comentarios.\n",
    "        children: Vec::new(),                           // Hijos.\n",
    "        level: None,                                    // Nivel.\n",
    "        depth: None,                                    // Profundidad/Especificidad.\n",
    "    };\n",
    "\n",
    "    let chunk = chunk.split(\"[Typedef]\").next().unwrap_or(chunk);       // Ignorar campos Typedef.\n",
    "\n",
    "    let lines: Vec<&str> = chunk\n",
    "        .lines()\n",
    "        .map(|l| l.trim())                               // eliminamos espacios a izquierda y derecha\n",
    "        .filter(|l| !l.is_empty())                       // quitamos líneas vacías\n",
    "        .collect();\n",
    "\n",
    "    if lines.is_empty() {\n",
    "        return None;\n",
    "    }\n",
    "    let mut valid = false;\n",
    "\n",
    "    // Verificación de campos que se obtienen de cada sección existente en Term.\n",
    "    for line in lines {\n",
    "        if line.starts_with(\"id: \") {                                               // Verificación para que sea válido (debe tener campo ID).\n",
    "            term.id = line[\"id: \".len()..].to_string();\n",
    "            valid = true;\n",
    "        } else if line.starts_with(\"name: \") {\n",
    "            term.name = line[\"name: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"namespace: \") {\n",
    "            term.namespace = line[\"namespace: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"def: \") {\n",
    "            term.definition = line[\"def: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"is_a: \") {\n",
    "            let parent = line[\"is_a: \".len()..]\n",
    "                .split_whitespace()\n",
    "                .next()\n",
    "                .unwrap_or(\"\")\n",
    "                .to_string();\n",
    "            if !parent.is_empty() {\n",
    "                term.parents.push(parent);\n",
    "            }\n",
    "        } else if line.starts_with(\"alt_id: \") {\n",
    "            term.alt_ids.push(line[\"alt_id: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"is_obsolete: true\") {\n",
    "            term.is_obsolete = true;\n",
    "        } else if line.starts_with(\"replaced_by: \") {\n",
    "            term.replaced_by = Some(line[\"replaced_by: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"consider: \") {\n",
    "            term.consider.push(line[\"consider: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"synonym: \") {\n",
    "            term.synonyms.push(line[\"synonym: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"xref: \") {\n",
    "            term.xrefs.push(line[\"xref: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"relationship: \") {\n",
    "            let rel_def = &line[\"relationship: \".len()..];\n",
    "            let mut parts = rel_def.split_whitespace();\n",
    "            if let (Some(rel), Some(target)) = (parts.next(), parts.next()) {\n",
    "                term.relationships.push((rel.to_string(), target.to_string()));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if valid {\n",
    "        Some(term)                                       // Si el termino es válido lo devuelve, en caso contrario se ignora.\n",
    "    } else {\n",
    "        None\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Compute the level and depth for each GO term in the ontology.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// terms : dict\n",
    "///   Mutable map of GO terms.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// None\n",
    "///   Updates the `level` and `depth` fields of each term in-place.\n",
    "pub fn compute_levels_and_depths(terms: &mut HashMap<String, GOTerm>) {\n",
    "    \n",
    "    // Paso 1: construir mapa de hijos teniendo en consideración las relaciones\n",
    "    // is_a para establecer jerarquía padre-hijo.\n",
    "    let mut child_map: HashMap<String, Vec<String>> = HashMap::default();\n",
    "    for (id, term) in terms.iter() {\n",
    "        for parent in &term.parents {\n",
    "            child_map.entry(parent.clone()).or_default().push(id.clone());\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Paso 2: inicializar level como una componente que identifica la distancia mínima\n",
    "    // entre el término raiz y el termino en estudio. Esto como función recursiva interna\n",
    "    // que selecciona camino según los padres de menor distancia.\n",
    "    fn init_level(\n",
    "        term_id: &str,\n",
    "        terms: &mut HashMap<String, GOTerm>,\n",
    "        visiting: &mut HashSet<String>,\n",
    "    ) -> usize {\n",
    "        if visiting.contains(term_id) {\n",
    "            // Ciclo detectado: se evita recursión infinita\n",
    "            eprintln!(\"⚠️ Ciclo detectado en level: {}\", term_id);\n",
    "            return 0;\n",
    "        }\n",
    "\n",
    "        if let Some(level) = terms.get(term_id).and_then(|t| t.level) {\n",
    "            return level;\n",
    "        }\n",
    "\n",
    "        visiting.insert(term_id.to_string());\n",
    "\n",
    "        let parents = terms\n",
    "            .get(term_id)\n",
    "            .map(|t| t.parents.clone())\n",
    "            .unwrap_or_default();\n",
    "\n",
    "        let level = if parents.is_empty() {\n",
    "            0\n",
    "        } else {\n",
    "            parents\n",
    "                .iter()\n",
    "                .map(|p| init_level(p, terms, visiting))\n",
    "                .min()\n",
    "                .unwrap_or(0) + 1\n",
    "        };\n",
    "\n",
    "        visiting.remove(term_id);\n",
    "        if let Some(term) = terms.get_mut(term_id) {\n",
    "            term.level = Some(level);\n",
    "        }\n",
    "\n",
    "        level\n",
    "    }\n",
    "\n",
    "    // Paso 3: inicializar depth como la distancia más larga a la raíz. Mismo concepto \n",
    "    // que punto anterior.\n",
    "    fn init_depth(\n",
    "        term_id: &str,\n",
    "        terms: &mut HashMap<String, GOTerm>,\n",
    "        visiting: &mut HashSet<String>,\n",
    "    ) -> usize {\n",
    "        if visiting.contains(term_id) {\n",
    "            eprintln!(\"Ciclo detectado en depth: {}\", term_id);\n",
    "            return 0;\n",
    "        }\n",
    "\n",
    "        if let Some(depth) = terms.get(term_id).and_then(|t| t.depth) {\n",
    "            return depth;\n",
    "        }\n",
    "\n",
    "        visiting.insert(term_id.to_string());\n",
    "\n",
    "        let parents = terms\n",
    "            .get(term_id)\n",
    "            .map(|t| t.parents.clone())\n",
    "            .unwrap_or_default();\n",
    "\n",
    "        let depth = if parents.is_empty() {\n",
    "            0\n",
    "        } else {\n",
    "            parents\n",
    "                .iter()\n",
    "                .map(|p| init_depth(p, terms, visiting))\n",
    "                .max()\n",
    "                .unwrap_or(0) + 1\n",
    "        };\n",
    "\n",
    "        visiting.remove(term_id);\n",
    "        if let Some(term) = terms.get_mut(term_id) {\n",
    "            term.depth = Some(depth);\n",
    "        }\n",
    "\n",
    "        depth\n",
    "    }\n",
    "\n",
    "    // Paso 4: recorrer todos los términos y calcular level + depth como llamados\n",
    "    // a las funciones por cada ids.\n",
    "    let ids: Vec<String> = terms.keys().cloned().collect();\n",
    "    for id in &ids {\n",
    "        let mut visiting = HashSet::default();\n",
    "        init_level(id, terms, &mut visiting);\n",
    "\n",
    "        let mut visiting = HashSet::default();\n",
    "        init_depth(id, terms, &mut visiting);\n",
    "    }\n",
    "\n",
    "    // Paso 5: rellenar el campo children con los hijos (solo vía is_a).\n",
    "    for (parent, children) in child_map {\n",
    "        if let Some(term) = terms.get_mut(&parent) {\n",
    "            term.children = children;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Download the latest GO OBO file if not present locally.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// None\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// str\n",
    "///   Path to the downloaded or existing OBO file.\n",
    "// Obtención de archivo .obo verificando si este existe en la ruta de destino, por defecto,\n",
    "// descarga la versión básica ¿Por qué se tomo esa decisión?.\n",
    "pub fn download_obo() -> Result<String, String> {\n",
    "    let obo_path = \"go-basic.obo\";\n",
    "    if Path::new(obo_path).exists() {\n",
    "        return Ok(obo_path.to_string());\n",
    "    }\n",
    "\n",
    "    let url = \"http://purl.obolibrary.org/obo/go/go-basic.obo\";\n",
    "    println!(\"Descargando ontología desde: {}\", url);\n",
    "    let response = get(url).map_err(|e| e.to_string())?;\n",
    "\n",
    "    let content = response.text().map_err(|e| e.to_string())?;\n",
    "    fs::write(obo_path, content).map_err(|e| e.to_string())?;\n",
    "\n",
    "    Ok(obo_path.to_string())\n",
    "}\n",
    "\n",
    "/// Load GO terms from an OBO file and cache them globally.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// path : Optional[str]\n",
    "///   Optional path to the OBO file.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of PyGOTerm\n",
    "///   List of GO terms as Python objects.\n",
    "// En esta función se realiza el llamado al parseo de GoTerms y su guardado en chache\n",
    "// para no necesitar re-ejecuciones de la función.\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (path=None))]\n",
    "pub fn load_go_terms(path: Option<String>) -> PyResult<Vec<PyGOTerm>> {\n",
    "    let path = path.unwrap_or_else(|| download_obo().unwrap());\n",
    "    let terms_map = parse_obo(&path);\n",
    "\n",
    "    // Guardar en la caché global\n",
    "    let _ = GO_TERMS_CACHE.set(RwLock::new(terms_map.clone()));\n",
    "\n",
    "    // Devolver lista de PyGOTerm (compatibilidad con Python).\n",
    "    let terms_vec = terms_map\n",
    "        .into_iter()\n",
    "        .map(|(_, v)| PyGOTerm::from(&v))\n",
    "        .collect();\n",
    "\n",
    "    Ok(terms_vec)\n",
    "}\n",
    "\n",
    "/// Load a GAF annotation file and cache the gene-to-GO mapping.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// path : str\n",
    "///   Path to the GAF file.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of GAFAnnotation\n",
    "///   List of parsed GAF annotations.\n",
    "// \n",
    "#[pyfunction]\n",
    "pub fn load_gaf(path: String) -> PyResult<Vec<GAFAnnotation>> {\n",
    "    // Se verifica acceso al archivo .gaf existente, en caso de que este no pueda ser \n",
    "    // accedido, entonces la función termina.\n",
    "    let file = File::open(&path)\n",
    "        .map_err(|e| pyo3::exceptions::PyIOError::new_err(e.to_string()))?;\n",
    "    let reader = BufReader::new(file);\n",
    "\n",
    "    // Get the loaded GO terms to check for obsolete terms.\n",
    "    // Recordar: Debieron haberse cargado en cache los go terms.\n",
    "    let terms = match crate::go_ontology::get_terms_or_error() {\n",
    "        Ok(t) => t,\n",
    "        Err(e) => return Err(e),\n",
    "    };\n",
    "\n",
    "    let mut annotations: Vec<GAFAnnotation> = Vec::new();                   // Vector de anotaciones de GAF (parte vacía).\n",
    "    let mut gene2go: HashMap<String, Vec<String>> = HashMap::default();     // Relación de genes a términos.\n",
    "\n",
    "    // Lectura linea a linea filtrando aquellas que no empiecen con '!'.\n",
    "    for line in reader.lines().filter_map(Result::ok).filter(|l| !l.starts_with('!')) {\n",
    "        let cols: Vec<&str> = line.split('\\t').collect();                   // Identificar columnas mediante una separación por tabuladores (son 7).\n",
    "        if cols.len() < 7 {                                                 // Debe tener como mínimo 7 campos como mínimo.\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // Columnas consideradas de cada linea válida.\n",
    "        let db_object_id = cols[1].to_string();\n",
    "        let qualifier = cols[3].to_string();\n",
    "        let mut go_term = cols[4].to_string();\n",
    "        let evidence = cols[6].to_string();\n",
    "        let gene = cols[2].to_string();\n",
    "\n",
    "        // Se ignoran registros de genes con terminos sin evidencia sustentada,\n",
    "        // asimismo de asociaciones del tipo \"El gen NO hace esto\".\n",
    "        // Filter out ND annotations.\n",
    "        if evidence == \"ND\" {\n",
    "            continue;\n",
    "        }\n",
    "        // Skip NOT annotations.\n",
    "        if qualifier.contains(\"NOT\") {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "\n",
    "        // Resolve obsolete terms\n",
    "        if let Some(term) = terms.get(&go_term) {\n",
    "            if term.is_obsolete {\n",
    "                if let Some(ref replacement) = term.replaced_by {\n",
    "                    // Use the replacement term instead\n",
    "                    go_term = replacement.clone();\n",
    "                } else if !term.consider.is_empty() {\n",
    "                    // If there are \"consider\" terms, choose the first one\n",
    "                    go_term = term.consider[0].clone();\n",
    "                } else {\n",
    "                    // If no replacement, drop the annotation\n",
    "                    continue;\n",
    "                }\n",
    "            }\n",
    "        } else {\n",
    "            // GO term not found at all\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // Add annotation.\n",
    "        annotations.push(GAFAnnotation {\n",
    "            db_object_id: db_object_id.clone(),\n",
    "            go_term: go_term.clone(),\n",
    "            evidence,\n",
    "        });\n",
    "\n",
    "        // Update gene -> GO mapping\n",
    "        gene2go.entry(gene).or_default().push(go_term);\n",
    "    }\n",
    "\n",
    "    // Save in global cache.\n",
    "    let _ = GENE2GO_CACHE.set(RwLock::new(gene2go));\n",
    "\n",
    "    Ok(annotations)\n",
    "}\n",
    "/// Build a term counter (counts, IC) from GAF annotations.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// py_annotations : list of GAFAnnotation\n",
    "///   List of GAFAnnotation Python objects.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// TermCounter\n",
    "///   Struct with counts and IC values.\n",
    "\n",
    "#[pyfunction]\n",
    "pub fn build_term_counter(\n",
    "    py: Python<'_>,\n",
    "    py_annotations: Vec<Py<GAFAnnotation>>,\n",
    ") -> PyResult<TermCounter> {\n",
    "    // Obtener los términos GO desde el caché global.\n",
    "    let terms = get_terms_or_error()?;\n",
    "\n",
    "    // Convertir las anotaciones de Py<GAFAnnotation> a GAFAnnotation (Rust).\n",
    "    let annotations: Vec<GAFAnnotation> = py_annotations\n",
    "        .into_iter()\n",
    "        .map(|py_ann| py_ann.extract(py))\n",
    "        .collect::<PyResult<_>>()?;\n",
    "\n",
    "    // Llamar a la función de conteo interna.\n",
    "    Ok(_build_term_counter(&annotations, &terms))\n",
    "}\n",
    "\n",
    "/// Internal: Build a term counter from Rust GAFAnnotation and GOTerm.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// annotations : list of GAFAnnotation\n",
    "///   List of GAFAnnotation structs.\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// TermCounter\n",
    "///   Struct with counts and IC values.\n",
    "fn _build_term_counter(\n",
    "    annotations: &[GAFAnnotation],\n",
    "    terms: &HashMap<String, GOTerm>,\n",
    ") -> TermCounter {\n",
    "    use rayon::prelude::*;                          // Motor de paralelismo.\n",
    "    use std::sync::Mutex;                           // Controlador de exclusión mutua.\n",
    "\n",
    "    // Parallel: build obj_to_terms - Objeto con toda la relación entre los términos y las anotaciones\n",
    "    // procesadas en el GAF.\n",
    "    let obj_to_terms: HashMap<&str, HashSet<String>> = {\n",
    "        let obj_to_terms_mutex: Mutex<HashMap<&str, HashSet<String>>> = Mutex::new(HashMap::default());\n",
    "        annotations.par_iter().for_each(|ann| {\n",
    "            let go_id = ann.go_term.as_str();\n",
    "            let mut term_set: HashSet<String> = collect_ancestors(go_id, terms);\n",
    "            term_set.insert(go_id.to_string());\n",
    "            let mut map = obj_to_terms_mutex.lock().unwrap();\n",
    "            map.entry(ann.db_object_id.as_str())\n",
    "                .or_default()\n",
    "                .extend(term_set);\n",
    "        });\n",
    "        obj_to_terms_mutex.into_inner().unwrap()\n",
    "    };\n",
    "\n",
    "    // Parallel: build counts and total_by_ns - Conteo de cada gen presente por termino y\n",
    "    // cantidad de genes únicos por ontología.\n",
    "    let (counts, total_by_ns) = {\n",
    "        let counts_mutex: Mutex<HashMap<String, usize>> = Mutex::new(HashMap::default());\n",
    "        let total_by_ns_mutex: Mutex<HashMap<String, usize>> = Mutex::new(HashMap::default());\n",
    "        obj_to_terms.values().collect::<Vec<_>>().par_iter().for_each(|term_ids| {\n",
    "            let mut namespaces_seen = HashSet::default();\n",
    "            for term_id in *term_ids {\n",
    "                if let Some(term) = terms.get(term_id.as_str()) {\n",
    "                    let mut counts = counts_mutex.lock().unwrap();\n",
    "                    *counts.entry(term_id.to_string()).or_insert(0) += 1;\n",
    "                    namespaces_seen.insert(term.namespace.as_str());\n",
    "                }\n",
    "            }\n",
    "            for ns in namespaces_seen {\n",
    "                let mut total_by_ns = total_by_ns_mutex.lock().unwrap();\n",
    "                *total_by_ns.entry(ns.to_string()).or_insert(0) += 1;\n",
    "            }\n",
    "        });\n",
    "        (counts_mutex.into_inner().unwrap(), total_by_ns_mutex.into_inner().unwrap())\n",
    "    };\n",
    "\n",
    "    // Calcular IC (secuencial, as it's fast).\n",
    "    let mut ic: HashMap<String, f64> = HashMap::default();\n",
    "    for (term_id, count) in &counts {\n",
    "        if let Some(term) = terms.get(term_id.as_str()) {\n",
    "            let total = total_by_ns.get(&term.namespace).copied().unwrap_or(1);\n",
    "            let freq = *count as f64 / total as f64;\n",
    "            let info_content = if freq > 0.0 { -freq.ln() } else { 0.0 };\n",
    "            ic.insert(term_id.clone(), info_content);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    TermCounter {\n",
    "        counts,\n",
    "        total_by_ns,\n",
    "        ic,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c3630",
   "metadata": {},
   "source": [
    "## Código 3 - go_ontology.rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18180c6c",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// ---------------- Librerias.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};       // Invocación de tablas hash.      \n",
    "use pyo3::prelude::*;                                               // Rust comprende Python (viceversa).\n",
    "use pyo3::types::PyString;                                          // Se invoca tipo de dato string de Python.\n",
    "use crate::go_loader::{GO_TERMS_CACHE, GENE2GO_CACHE};              // Se obtiene la cache generada en go_loader (si existe).\n",
    "use pyo3::exceptions::PyValueError;                                 // Se invoca 'error por valor' de Python.\n",
    "\n",
    "/// Struct representing a Gene Ontology (GO) term.\n",
    "///\n",
    "/// Fields\n",
    "/// ------\n",
    "/// id : str\n",
    "///   GO term identifier (e.g., GO:0006397).\n",
    "/// name : str\n",
    "///   Human-readable name of the term.\n",
    "/// namespace : str\n",
    "///   Ontology namespace (e.g., biological_process).\n",
    "/// definition : str\n",
    "///   Textual definition of the term.\n",
    "/// parents : list of str\n",
    "///   List of parent GO term IDs (is_a relationships).\n",
    "/// children : list of str\n",
    "///   List of child GO term IDs (is_a relationships).\n",
    "/// depth : Optional[int]\n",
    "///   Maximum distance to a root term (None if not computed).\n",
    "/// level : Optional[int]\n",
    "///   Minimum distance to a root term (None if not computed).\n",
    "/// is_obsolete : bool\n",
    "///   True if the term is obsolete.\n",
    "/// alt_ids : list of str\n",
    "///   Alternative GO IDs for this term.\n",
    "/// replaced_by : Optional[str]\n",
    "///   If obsolete, the term that replaces this one.\n",
    "/// consider : list of str\n",
    "///   Suggested replacement terms if obsolete.\n",
    "/// synonyms : list of str\n",
    "///   List of synonyms.\n",
    "/// xrefs : list of str\n",
    "///   Cross-references to other databases.\n",
    "/// relationships : list of (str, str)\n",
    "///   Other relationships (e.g., part_of).\n",
    "/// comment : Optional[str]\n",
    "///   Additional comments.\n",
    "#[derive(Clone)]\n",
    "pub struct GOTerm {\n",
    "    pub id: String,\n",
    "    pub name: String,\n",
    "    pub namespace: String,\n",
    "    pub definition: String,\n",
    "    pub parents: Vec<String>,\n",
    "    pub children: Vec<String>,\n",
    "    pub depth: Option<usize>,\n",
    "    pub level: Option<usize>,\n",
    "    pub is_obsolete: bool,\n",
    "    pub alt_ids: Vec<String>,\n",
    "    pub replaced_by: Option<String>,\n",
    "    pub consider: Vec<String>,\n",
    "    pub synonyms: Vec<String>,\n",
    "    pub xrefs: Vec<String>,\n",
    "    pub relationships: Vec<(String, String)>,\n",
    "    pub comment: Option<String>,\n",
    "}\n",
    "\n",
    "/// Python-exposed struct representing a GO term (for use in the Python API).\n",
    "///\n",
    "/// Fields\n",
    "/// ------\n",
    "/// id : str\n",
    "///   GO term identifier.\n",
    "/// name : str\n",
    "///   Human-readable name of the term.\n",
    "/// namespace : str\n",
    "///   Ontology namespace.\n",
    "/// definition : str\n",
    "///   Textual definition of the term.\n",
    "/// parents : list of str\n",
    "///   List of parent GO term IDs.\n",
    "/// children : list of str\n",
    "///   List of child GO term IDs.\n",
    "/// depth : Optional[int]\n",
    "///   Maximum distance to a root term.\n",
    "/// level : Optional[int]\n",
    "///   Minimum distance to a root term.\n",
    "/// is_obsolete : bool\n",
    "///   True if the term is obsolete.\n",
    "/// alt_ids : list of str\n",
    "///   Alternative GO IDs for this term.\n",
    "/// replaced_by : Optional[str]\n",
    "///   If obsolete, the term that replaces this one.\n",
    "/// consider : list of str\n",
    "///   Suggested replacement terms if obsolete.\n",
    "/// synonyms : list of str\n",
    "///   List of synonyms.\n",
    "/// xrefs : list of str\n",
    "///   Cross-references to other databases.\n",
    "/// relationships : list of (str, str)\n",
    "///   Other relationships (e.g., part_of).\n",
    "/// comment : Optional[str]\n",
    "///   Additional comments.\n",
    "#[pyclass]\n",
    "#[derive(Clone)]\n",
    "pub struct PyGOTerm {\n",
    "    #[pyo3(get)] pub id: String,\n",
    "    #[pyo3(get)] pub name: String,\n",
    "    #[pyo3(get)] pub namespace: String,\n",
    "    #[pyo3(get)] pub definition: String,\n",
    "    #[pyo3(get)] pub parents: Vec<String>,\n",
    "    #[pyo3(get)] pub children: Vec<String>,\n",
    "    #[pyo3(get)] pub depth: Option<usize>,\n",
    "    #[pyo3(get)] pub level: Option<usize>,\n",
    "    #[pyo3(get)] pub is_obsolete: bool,\n",
    "    #[pyo3(get)] pub alt_ids: Vec<String>,\n",
    "    #[pyo3(get)] pub replaced_by: Option<String>,\n",
    "    #[pyo3(get)] pub consider: Vec<String>,\n",
    "    #[pyo3(get)] pub synonyms: Vec<String>,\n",
    "    #[pyo3(get)] pub xrefs: Vec<String>,\n",
    "    #[pyo3(get)] pub relationships: Vec<(String, String)>,\n",
    "    #[pyo3(get)] pub comment: Option<String>,\n",
    "}\n",
    "\n",
    "// Esta sección de código define como se debe realizar la copia de los\n",
    "// datos de la estructura GoTerm de Rust a su versión de Python de forma\n",
    "// en que no existan errores de compatibilidad\n",
    "impl From<&GOTerm> for PyGOTerm {\n",
    "    fn from(term: &GOTerm) -> Self {\n",
    "        Self {\n",
    "            id: term.id.clone(),\n",
    "            name: term.name.clone(),\n",
    "            namespace: term.namespace.clone(),\n",
    "            definition: term.definition.clone(),\n",
    "            parents: term.parents.clone(),\n",
    "            children: term.children.clone(),\n",
    "            depth: term.depth,\n",
    "            level: term.level,\n",
    "            is_obsolete: term.is_obsolete,\n",
    "            alt_ids: term.alt_ids.clone(),\n",
    "            replaced_by: term.replaced_by.clone(),\n",
    "            consider: term.consider.clone(),\n",
    "            synonyms: term.synonyms.clone(),\n",
    "            xrefs: term.xrefs.clone(),\n",
    "            relationships: term.relationships.clone(),\n",
    "            comment: term.comment.clone(),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Esta sección de código define el comportamiento de Python a imprimir\n",
    "// un término GO como un mensaje que considera los elementos definidos\n",
    "// previamente. Además de automatizar la escritura del nombre del\n",
    "// objeto.\n",
    "#[pymethods]\n",
    "impl PyGOTerm {\n",
    "    fn __repr__(slf: &Bound<'_, Self>) -> PyResult<String> {\n",
    "        let class_name: Bound<'_, PyString> = slf.get_type().qualname()?;\n",
    "        let s = slf.borrow();\n",
    "        Ok(format!(\n",
    "            \"{} id: {}\\nname: {}\\nnamespace: {}\\ndefinition: {}\\nparents: {:?}\\nchildren: {:?}\\ndepth: {:?}\\nlevel: {:?}\\nis_obsolete: {}\\nalt_ids: {:?}\\nreplaced_by: {:?}\\nconsider: {:?}\\nsynonyms: {:?}\\nxrefs: {:?}\\nrelationships: {:?}\\ncomments: {:?}\",\n",
    "            class_name, s.id, s.name, s.namespace, s.definition, s.parents, s.children, s.depth, s.level,\n",
    "            s.is_obsolete, s.alt_ids, s.replaced_by, s.consider, s.synonyms, s.xrefs, s.relationships, s.comment\n",
    "        ))\n",
    "    }\n",
    "}\n",
    "\n",
    "// Esta sección genera un ciclo de regreso de la versión Python a la versión\n",
    "// de Rust, esto hace que cualquier modificación de una capa afecte a otra.\n",
    "impl From<PyGOTerm> for GOTerm {\n",
    "    fn from(py_term: PyGOTerm) -> Self {\n",
    "        Self {\n",
    "            id: py_term.id,\n",
    "            name: py_term.name,\n",
    "            namespace: py_term.namespace,\n",
    "            definition: py_term.definition,\n",
    "            parents: py_term.parents,\n",
    "            children: py_term.children,\n",
    "            depth: py_term.depth,\n",
    "            level: py_term.level,\n",
    "            is_obsolete: py_term.is_obsolete,\n",
    "            alt_ids: py_term.alt_ids,\n",
    "            replaced_by: py_term.replaced_by,\n",
    "            consider: py_term.consider,\n",
    "            synonyms: py_term.synonyms,\n",
    "            xrefs: py_term.xrefs,\n",
    "            relationships: py_term.relationships,\n",
    "            comment: py_term.comment,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Las posteriores 2 secciones verifican si es posible acceder al cache de los\n",
    "// Go terms guardados y la relación de genes a terminos GO, estableciendo un\n",
    "// error RUNTIMEERROR en caso de no encontrarlos.\n",
    "\n",
    "/// Get a read lock on the global GO terms map, or error if not loaded.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// None\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// RwLockReadGuard<HashMap<String, GOTerm>>\n",
    "///   Read guard for the GO terms map.\n",
    "pub fn get_terms_or_error<'a>() -> PyResult<parking_lot::RwLockReadGuard<'a, HashMap<String, GOTerm>>> {\n",
    "    Ok(\n",
    "        GO_TERMS_CACHE\n",
    "            .get()\n",
    "            .ok_or_else(|| pyo3::exceptions::PyRuntimeError::new_err(\"GO terms not loaded. Call go3.load_go_terms() first.\"))?\n",
    "            .read()\n",
    "    )\n",
    "}\n",
    "\n",
    "/// Get a read lock on the global gene-to-GO mapping, or error if not loaded.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// None\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// RwLockReadGuard<HashMap<String, Vec<String>>>\n",
    "///   Read guard for the gene2go map.\n",
    "pub fn get_gene2go_or_error<'a>() -> PyResult<parking_lot::RwLockReadGuard<'a, HashMap<String, Vec<String>>>> {\n",
    "    Ok(\n",
    "        GENE2GO_CACHE\n",
    "            .get()\n",
    "            .ok_or_else(|| pyo3::exceptions::PyRuntimeError::new_err(\"Gene2GO mapping not loaded. Call go3.load_gene2go() first.\"))?\n",
    "            .read()\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "/// Get the PyGOTerm object for a given GO term ID.\n",
    "///\n",
    "/// Raises:\n",
    "///     ValueError: If the GO term does not exist in the ontology.\n",
    "#[pyfunction]\n",
    "pub fn get_term_by_id(go_id: &str) -> PyResult<PyGOTerm> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    match terms.get(go_id) {\n",
    "        Some(term) => Ok(PyGOTerm::from(term)),\n",
    "        None => Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id\n",
    "        ))),\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Collect all ancestors of a GO term (recursively via is_a).\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id : str\n",
    "///   GO term ID.\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// HashSet<String>\n",
    "///   Set of ancestor GO term IDs.\n",
    "pub fn collect_ancestors(go_id: &str, terms: &HashMap<String, GOTerm>) -> HashSet<String> {\n",
    "    // Try to use the precomputed cache if available\n",
    "    if let Some(lock) = crate::go_loader::ANCESTORS_CACHE.get() {\n",
    "        let cache = lock.read();\n",
    "        if let Some(ancestors) = cache.get(go_id) {\n",
    "            return ancestors.clone();\n",
    "        }\n",
    "    }\n",
    "    // Fallback: original computation\n",
    "    let mut visited = HashSet::default();\n",
    "    let mut stack = vec![go_id];\n",
    "    while let Some(current) = stack.pop() {\n",
    "        if visited.insert(current.to_string()) {\n",
    "            if let Some(term) = terms.get(current) {\n",
    "                for parent in &term.parents {\n",
    "                    stack.push(parent);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "//    visited.remove(go_id);\n",
    "    visited\n",
    "}\n",
    "\n",
    "/// Returns the list of all ancestors in the ontology for the given GO Term.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id : str\n",
    "///   GO term ID.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of str\n",
    "///   List of IDs of all the ancestors in the ontology.\n",
    "// Esto cuenta como un encapsulamiento del código con la finalidad de que\n",
    "// solo el usuario provea el GO id.\n",
    "#[pyfunction]\n",
    "pub fn ancestors(go_id: &str) -> PyResult<Vec<String>> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let visited = collect_ancestors(go_id, &terms);\n",
    "    Ok(visited.into_iter().collect())\n",
    "}\n",
    "\n",
    "/// Returns the list of all the common ancestors in the ontology for the given GO Terms.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id1 : str\n",
    "///   GO term ID 1.\n",
    "/// go_id2 : str\n",
    "///   GO term ID 2.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of str\n",
    "///   List of IDs of all the common ancestors in the ontology.\n",
    "// Esta función lo que realiza es la intersección de todos los términos\n",
    "// ancestrales entre dos GO ids.\n",
    "#[pyfunction]\n",
    "pub fn common_ancestor(go_id1: &str, go_id2: &str) -> PyResult<Vec<String>> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let set1 = collect_ancestors(go_id1, &terms);\n",
    "    let set2 = collect_ancestors(go_id2, &terms);\n",
    "    let mut common: Vec<String> = set1.intersection(&set2).map(|s| (*s).to_string()).collect();\n",
    "    common.sort_unstable();\n",
    "    Ok(common)\n",
    "}\n",
    "\n",
    "/// Returns the deepest common ancestor in the ontology for the given GO Terms.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id1 : str\n",
    "///   GO term ID 1.\n",
    "/// go_id2 : str\n",
    "///   GO term ID 2.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// Option<String>\n",
    "///   ID of the deepest common ancestor in the ontology.\n",
    "#[pyfunction]\n",
    "pub fn deepest_common_ancestor(go_id1: &str, go_id2: &str) -> PyResult<Option<String>> {\n",
    "    let terms = get_terms_or_error()?;                                      // Se buscan términos dentro de la cache existente.\n",
    "\n",
    "    if !terms.contains_key(go_id1) {                                        // Verificación de que los terminos existan dentro del código.\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id1\n",
    "        )));\n",
    "    }\n",
    "    if !terms.contains_key(go_id2) {\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id2\n",
    "        )));\n",
    "    }\n",
    "\n",
    "    // Se establece el par ordenado de ambos ids, ya que esto estandariza\n",
    "    // los pares para consulta en cache, como estandarizar que el valor\n",
    "    // es independiente del orden.\n",
    "    let (id_a, id_b) = if go_id1 <= go_id2 {\n",
    "        (go_id1, go_id2)\n",
    "    } else {\n",
    "        (go_id2, go_id1)\n",
    "    };\n",
    "\n",
    "    // Try to use the DCA cache if available.\n",
    "    if let Some(lock) = crate::go_loader::DCA_CACHE.get() {\n",
    "        let cache = lock.write();\n",
    "        if let Some(result) = cache.get(&(id_a.to_string(), id_b.to_string())) {\n",
    "            return Ok(Some(result.clone()));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // En caso de no encontrar ancestro en caché.\n",
    "    let set1 = collect_ancestors(id_a, &terms);                             // Ancestros del id_a.\n",
    "    let set2 = collect_ancestors(id_b, &terms);                             // Ancestros del id_b.\n",
    "    let mut best = None;\n",
    "    let mut max_depth = 0;\n",
    "    for term_id in set1.intersection(&set2) {                               // Intersección entre ambas listas de ancestros.\n",
    "        if let Some(term) = terms.get(term_id) {                            // Se verifican campos depth para obtener el DCA.\n",
    "            if let Some(depth) = term.depth {\n",
    "                if depth >= max_depth {\n",
    "                    max_depth = depth;\n",
    "                    best = Some(term_id.to_string());\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Store the result in the cache if available.\n",
    "    if let Some(lock) = crate::go_loader::DCA_CACHE.get() {\n",
    "        let mut cache = lock.write();\n",
    "        if let Some(ref dca) = best {\n",
    "            cache.insert((id_a.to_string(), id_b.to_string()), dca.clone());\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Ok(best)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f6a07",
   "metadata": {},
   "source": [
    "## Código 4 - semantic.rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e47c0",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// -------------------- Librerias \n",
    "use pyo3::exceptions::PyValueError;                                             // Se solicita a Python la invocación del ValueError.\n",
    "use pyo3::prelude::*;                                                           // Rust comprende Python (viceversa).\n",
    "use rayon::prelude::*;                                                          // Administración de funciones con comportamiento paralelo.                                                  \n",
    "use crate::go_loader::TermCounter;                                              // Invocación de la estructura de conteo de términos.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};                   // Tablas Hash optimizadas.\n",
    "// Petición de componentes al archivo go_ontology.\n",
    "use crate::go_ontology::{deepest_common_ancestor, get_term_by_id, get_terms_or_error, get_gene2go_or_error};\n",
    "use dashmap::DashMap;                                                           // Manejo concurrente de tablas hash.\n",
    "use rayon::ThreadPoolBuilder;                                                   // Administración de nucleos para trabajo paralelo.\n",
    "\n",
    "/// Configure the maximum number of threads rayon will use.\n",
    "///\n",
    "/// Args:\n",
    "///     n_threads (int): Number of threads to use. If 0, uses all available cores.\n",
    "#[pyfunction]\n",
    "pub fn set_num_threads(n_threads: usize) -> PyResult<()> {\n",
    "    if n_threads == 0 {\n",
    "        ThreadPoolBuilder::new()\n",
    "            .build_global()\n",
    "            .map_err(|e| pyo3::exceptions::PyRuntimeError::new_err(e.to_string()))?;\n",
    "    } else {\n",
    "        ThreadPoolBuilder::new()\n",
    "            .num_threads(n_threads)\n",
    "            .build_global()\n",
    "            .map_err(|e| pyo3::exceptions::PyRuntimeError::new_err(e.to_string()))?;\n",
    "    }\n",
    "    Ok(())\n",
    "}\n",
    "\n",
    "/// Compute the Information Content (IC) of a GO term.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id : str\n",
    "///   GO term identifier.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed term counter with IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   The IC of the GO term.\n",
    "#[pyfunction]\n",
    "#[pyo3(text_signature = \"(go_id, counter)\")]\n",
    "pub fn term_ic(go_id: &str, counter: &TermCounter) -> f64 {\n",
    "    *counter.ic.get(go_id).unwrap_or(&0.0)\n",
    "}\n",
    "\n",
    "// Esta sección de código actua como un menú adaptativo dedicado a la invocación\n",
    "// de los métodos de similitud entre elementos, incluso, logrando reconocer\n",
    "// opciones aunque el usuario cometa errores ortograficos.\n",
    "#[derive(Debug)]\n",
    "enum SimilarityMethod {\n",
    "    Resnik,\n",
    "    Lin,\n",
    "    JC,\n",
    "    SimRel,\n",
    "    ICCoef,\n",
    "    GraphIC,\n",
    "    Wang,\n",
    "    TopoICSim,\n",
    "}\n",
    "\n",
    "// Dentro de esta subsección se invocan los distintos tipos de méticas de similitud ya sea\n",
    "// por medio de términos GO, lo bueno de esta función es que ya existen elementos construidos\n",
    "// que fueron verificados, ya se puede asegurar que su implementación es correcta.\n",
    "// Dentro de esta sección solo se implementan otras componentes de protección para establecer el\n",
    "// resultado como 0.\n",
    "impl SimilarityMethod {\n",
    "    fn from_str(name: &str) -> Option<Self> {\n",
    "        match name.to_lowercase().as_str() {\n",
    "            \"resnik\" => Some(SimilarityMethod::Resnik),\n",
    "            \"lin\" => Some(SimilarityMethod::Lin),\n",
    "            \"jc\" => Some(SimilarityMethod::JC),\n",
    "            \"simrel\" => Some(SimilarityMethod::SimRel),\n",
    "            \"iccoef\" => Some(SimilarityMethod::ICCoef),\n",
    "            \"graphic\" => Some(SimilarityMethod::GraphIC),\n",
    "            \"wang\" => Some(SimilarityMethod::Wang),\n",
    "            \"topoicsim\" => Some(SimilarityMethod::TopoICSim),\n",
    "            _ => None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fn compute(&self, id1: &str, id2: &str, counter: &TermCounter) -> f64 {\n",
    "        match self {\n",
    "            SimilarityMethod::Resnik => {\n",
    "                let dca = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => dca,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                *counter.ic.get(&dca).unwrap_or(&0.0)\n",
    "            }\n",
    "            SimilarityMethod::Lin => {\n",
    "                let dca = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => dca,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                if id1 == id2 {\n",
    "                    return 1.0\n",
    "                }\n",
    "                let resnik = *counter.ic.get(&dca).unwrap_or(&0.0);\n",
    "                if resnik == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                let ic1 = *counter.ic.get(id1).unwrap_or(&0.0);\n",
    "                let ic2 = *counter.ic.get(id2).unwrap_or(&0.0);\n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                2.0 * resnik / (ic1 + ic2)\n",
    "            }\n",
    "            SimilarityMethod::JC => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                let distance = ic1 + ic2 - 2.0 * dca_ic;\n",
    "                if distance <= 0.0 {\n",
    "                    return f64::INFINITY;  // Máxima similitud\n",
    "                }\n",
    "                if distance.is_infinite() {\n",
    "                    0.0\n",
    "                } else {\n",
    "                    1.0 / (1.0 + distance)\n",
    "                }\n",
    "            }\n",
    "            SimilarityMethod::SimRel => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if dca_ic == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let lin = (2.0 * dca_ic) / (ic1 + ic2);\n",
    "                lin * (1.0 - (-dca_ic).exp())\n",
    "            }\n",
    "            SimilarityMethod::ICCoef => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                dca_ic / ic1.min(ic2)\n",
    "            }\n",
    "            SimilarityMethod::GraphIC => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let depth1 = t1.depth.unwrap_or(0);\n",
    "                let depth2 = t2.depth.unwrap_or(0);\n",
    "                let max_depth = (depth1.max(depth2) + 1) as f64;\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                dca_ic / max_depth\n",
    "            }\n",
    "            SimilarityMethod::Wang => {\n",
    "                let terms = match crate::go_loader::GO_TERMS_CACHE.get() {\n",
    "                    Some(lock) => lock.read(),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                let terms = &*terms;\n",
    "            \n",
    "                let t1 = match terms.get(id1) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t2 = match terms.get(id2) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let sv_a = semantic_contributions(id1, terms);\n",
    "                let sv_b = semantic_contributions(id2, terms);\n",
    "            \n",
    "                let sum_a: f64 = sv_a.values().sum();\n",
    "                let sum_b: f64 = sv_b.values().sum();\n",
    "            \n",
    "                let common_keys: std::collections::HashSet<_> = sv_a.keys().collect::<HashSet<_>>()\n",
    "                    .intersection(&sv_b.keys().collect::<HashSet<_>>())\n",
    "                    .cloned()\n",
    "                    .collect();\n",
    "            \n",
    "                let mut numerator = 0.0;\n",
    "                for key in common_keys {\n",
    "                    if let (Some(w1), Some(w2)) = (sv_a.get(key), sv_b.get(key)) {\n",
    "                        numerator += (*w1).min(*w2);\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "                if sum_a + sum_b == 0.0 {\n",
    "                    0.0\n",
    "                } else {\n",
    "                    numerator / ((sum_a + sum_b) / 2.0)\n",
    "                }\n",
    "            }\n",
    "            SimilarityMethod::TopoICSim => {\n",
    "                // Get terms and check namespace\n",
    "                let terms = match crate::go_loader::GO_TERMS_CACHE.get() {\n",
    "                    Some(lock) => lock.read(),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t1 = match terms.get(id1) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t2 = match terms.get(id2) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Disjunctive common ancestors\n",
    "                let dca_set = disjunctive_common_ancestors(id1, id2, &terms);\n",
    "                if dca_set.is_empty() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Find all roots\n",
    "                let roots = find_roots(&terms);\n",
    "                if roots.is_empty() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                let mut min_d = f64::INFINITY;\n",
    "                for x in dca_set {\n",
    "                    // Weighted shortest path from t1 to x and t2 to x\n",
    "                    let wsp1 = weighted_shortest_path_iic(id1, &x, &terms, counter);\n",
    "                    let wsp2 = weighted_shortest_path_iic(id2, &x, &terms, counter);\n",
    "                    if wsp1.is_none() || wsp2.is_none() {\n",
    "                        continue;\n",
    "                    }\n",
    "                    let wsp = wsp1.unwrap() + wsp2.unwrap();\n",
    "                    // Weighted longest path from x to any root (take the max over all roots)\n",
    "                    let mut max_wlp = None;\n",
    "                    for root in &roots {\n",
    "                        if let Some(wlp) = weighted_longest_path_iic(&x, root, &terms, counter) {\n",
    "                            max_wlp = Some(max_wlp.map_or(wlp, |m: f64| m.max(wlp)));\n",
    "                        }\n",
    "                    }\n",
    "                    let wlp = match max_wlp {\n",
    "                        Some(val) if val > 0.0 => val,\n",
    "                        _ => continue,\n",
    "                    };\n",
    "                    let d = wsp / wlp;\n",
    "                    if d < min_d {\n",
    "                        min_d = d;\n",
    "                    }\n",
    "                }\n",
    "                if !min_d.is_finite() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Similarity formula: 1 - (arctan(D) / (pi/2))\n",
    "                let sim = 1.0 - (min_d.atan() / (std::f64::consts::FRAC_PI_2));\n",
    "                if sim.is_finite() && sim > 0.0 { sim } else { 0.0 }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Compute semantic similarity between two GO terms using a selected method.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// id1 : str\n",
    "///   First GO term ID.\n",
    "/// id2 : str\n",
    "///   Second GO term ID.\n",
    "/// method : str\n",
    "///   Name of the similarity method. Options: \"resnik\", \"lin\", etc.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   Similarity score.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If the method is unknown.\n",
    "#[pyfunction]\n",
    "pub fn semantic_similarity(\n",
    "    id1: &str,\n",
    "    id2: &str,\n",
    "    method: &str,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<f64> {\n",
    "\n",
    "    let method_enum = SimilarityMethod::from_str(method)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", method)))?;\n",
    "\n",
    "    Ok(method_enum.compute(id1, id2, counter))\n",
    "}\n",
    "\n",
    "/// Compute pairwise semantic similarity in batch using a selected method.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// list1 : list of str\n",
    "///   First list of GO term IDs.\n",
    "/// list2 : list of str\n",
    "///   Second list of GO term IDs.\n",
    "/// method : str\n",
    "///   Name of the similarity method.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of float\n",
    "///   List of similarity scores.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If input lists differ in length or method is unknown.\n",
    "#[pyfunction]\n",
    "pub fn batch_similarity(\n",
    "    list1: Vec<String>,\n",
    "    list2: Vec<String>,\n",
    "    method: &str,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<Vec<f64>> {\n",
    "    if list1.len() != list2.len() {\n",
    "        return Err(PyValueError::new_err(\"Both lists must be the same length\"));\n",
    "    }\n",
    "\n",
    "    let method_enum = SimilarityMethod::from_str(method)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", method)))?;\n",
    "\n",
    "    // 1. Collect all unique pairs (order them to avoid (a,b) vs (b,a) duplicates)\n",
    "    let unique_pairs: HashSet<(String, String)> = list1.iter().zip(list2.iter())\n",
    "        .map(|(a, b)| {\n",
    "            if a <= b {\n",
    "                (a.clone(), b.clone())\n",
    "            } else {\n",
    "                (b.clone(), a.clone())\n",
    "            }\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    // 2. Compute similarity for each unique pair in parallel\n",
    "    let sim_map: HashMap<(String, String), f64> = unique_pairs\n",
    "        .par_iter()\n",
    "        .map(|(a, b)| {\n",
    "            let sim = method_enum.compute(a, b, counter);\n",
    "            ((a.clone(), b.clone()), sim)\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    // 3. For each original pair, look up the result (parallelized)\n",
    "    let result: Vec<f64> = list1.par_iter().zip(list2.par_iter())\n",
    "        .map(|(a, b)| {\n",
    "            let key = if a <= b { (a.clone(), b.clone()) } else { (b.clone(), a.clone()) };\n",
    "            *sim_map.get(&key).unwrap_or(&0.0)\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    Ok(result)\n",
    "}\n",
    "\n",
    "\n",
    "/// Compute semantic similarity between genes.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// gene1 : str\n",
    "///   Gene symbol of the first gene.\n",
    "/// gene2 : str\n",
    "///   Gene symbol of the second gene.\n",
    "/// ontology : str\n",
    "///   Name of the subontology of GO to use: BP, MF or CC.\n",
    "/// similarity : str\n",
    "///   Name of the similarity method.\n",
    "/// groupwise : str\n",
    "///   Combination method to generate the similarities between genes. Options: \"bma\", \"max\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   Similarity score.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If method or combine are unknown.\n",
    "#[pyfunction]\n",
    "pub fn compare_genes(\n",
    "    gene1: &str,\n",
    "    gene2: &str,\n",
    "    ontology: String,\n",
    "    similarity: &str,\n",
    "    groupwise: String,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<f64> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let gene2go = get_gene2go_or_error()?;\n",
    "    let g1_terms = gene2go.get(gene1).ok_or_else(|| {\n",
    "        pyo3::exceptions::PyValueError::new_err(format!(\"Gene '{}' not found in mapping\", gene1))\n",
    "    })?;\n",
    "    let g2_terms = gene2go.get(gene2).ok_or_else(|| {\n",
    "        pyo3::exceptions::PyValueError::new_err(format!(\"Gene '{}' not found in mapping\", gene2))\n",
    "    })?;\n",
    "    let ns = match ontology.as_str() {\n",
    "        \"BP\" => \"biological_process\",\n",
    "        \"MF\" => \"molecular_function\",\n",
    "        \"CC\" => \"cellular_component\",\n",
    "        _ => {\n",
    "            return Err(PyValueError::new_err(format!(\n",
    "                \"Invalid ontology '{}'. Must be 'BP', 'MF', or 'CC'\",\n",
    "                ontology\n",
    "            )))\n",
    "        }\n",
    "    };\n",
    "    let f1: Vec<String> = g1_terms\n",
    "        .iter()\n",
    "        .filter(|id| terms.get(*id).map_or(false, |t| t.namespace.to_ascii_lowercase() == ns))\n",
    "        .cloned()\n",
    "        .collect();\n",
    "\n",
    "    let f2: Vec<String> = g2_terms\n",
    "        .iter()\n",
    "        .filter(|id| terms.get(*id).map_or(false, |t| t.namespace.to_ascii_lowercase() == ns))\n",
    "        .cloned()\n",
    "        .collect();\n",
    "    print!(\"{:?}\", f1);\n",
    "    print!(\"{:?}\", f2);\n",
    "    if f1.is_empty() || f2.is_empty() {\n",
    "        return Ok(0.0);\n",
    "    }\n",
    "\n",
    "    let sim_fn = SimilarityMethod::from_str(similarity)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", similarity)))?;\n",
    "\n",
    "    let score = match groupwise.as_str() {\n",
    "        \"max\" => {\n",
    "            f1.par_iter()\n",
    "                .map(|id1| {\n",
    "                    f2.par_iter()\n",
    "                        .map(|id2| sim_fn.compute(id1, id2, counter))\n",
    "                        .reduce(|| 0.0, f64::max)\n",
    "                })\n",
    "                .reduce(|| 0.0, f64::max)\n",
    "        }\n",
    "        \"bma\" => {\n",
    "            let sem1: Vec<f64> = f1.par_iter()\n",
    "                .map(|id1| {\n",
    "                    f2.par_iter()\n",
    "                        .map(|id2| sim_fn.compute(id1, id2, counter))\n",
    "                        .reduce(|| 0.0, f64::max)\n",
    "                })\n",
    "                .collect();\n",
    "\n",
    "            let sem2: Vec<f64> = f2.par_iter()\n",
    "                .map(|id2| {\n",
    "                    f1.par_iter()\n",
    "                        .map(|id1| sim_fn.compute(id1, id2, counter))\n",
    "                        .reduce(|| 0.0, f64::max)\n",
    "                })\n",
    "                .collect();\n",
    "\n",
    "            let total = sem1.len() + sem2.len();\n",
    "            if total == 0 {\n",
    "                0.0\n",
    "            } else {\n",
    "                (sem1.iter().sum::<f64>() + sem2.iter().sum::<f64>()) / total as f64\n",
    "            }\n",
    "        }\n",
    "        _ => return Err(pyo3::exceptions::PyValueError::new_err(\"Unknown groupwise strategy\")),\n",
    "    };\n",
    "\n",
    "    Ok(score)\n",
    "}\n",
    "\n",
    "/// Compute semantic similarity between genes in batches.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// pairs : list of (str, str)\n",
    "///   List of pairs of genes to calculate the semantic similarity\n",
    "/// ontology : str\n",
    "///   Name of the subontology of GO to use: BP, MF or CC.\n",
    "/// similarity : str\n",
    "///   Name of the similarity method.\n",
    "/// groupwise : str\n",
    "///   Combination method to generate the similarities between genes. Options: \"bma\", \"max\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of float\n",
    "///   List of similarity scores.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If method or combine are unknown.\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (pairs, ontology, similarity, groupwise, counter))]\n",
    "pub fn compare_gene_pairs_batch(\n",
    "    pairs: Vec<(String, String)>,\n",
    "    ontology: String,\n",
    "    similarity: &str,\n",
    "    groupwise: String,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<Vec<f64>> {\n",
    "    let gene2go = get_gene2go_or_error()?;\n",
    "    let terms = get_terms_or_error()?;\n",
    "\n",
    "    let ns = match ontology.as_str() {\n",
    "        \"BP\" => \"biological_process\",\n",
    "        \"MF\" => \"molecular_function\",\n",
    "        \"CC\" => \"cellular_component\",\n",
    "        _ => {\n",
    "            return Err(PyValueError::new_err(format!(\n",
    "                \"Invalid ontology '{}'. Must be 'BP', 'MF', or 'CC'\",\n",
    "                ontology\n",
    "            )))\n",
    "        }\n",
    "    };\n",
    "\n",
    "    let sim_fn = SimilarityMethod::from_str(similarity)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", similarity)))?;\n",
    "\n",
    "    let scores: Vec<f64> = pairs\n",
    "        .into_par_iter()\n",
    "        .map(|(g1, g2)| {\n",
    "            let go1: Vec<_> = gene2go\n",
    "                .get(&g1)\n",
    "                .into_iter()\n",
    "                .flatten()\n",
    "                .filter(|go| terms.get(go.as_str()).map_or(false, |t| t.namespace.eq_ignore_ascii_case(ns)))\n",
    "                .cloned()\n",
    "                .collect();\n",
    "\n",
    "            let go2: Vec<_> = gene2go\n",
    "                .get(&g2)\n",
    "                .into_iter()\n",
    "                .flatten()\n",
    "                .filter(|go| terms.get(go.as_str()).map_or(false, |t| t.namespace.eq_ignore_ascii_case(ns)))\n",
    "                .cloned()\n",
    "                .collect();\n",
    "\n",
    "            if go1.is_empty() || go2.is_empty() {\n",
    "                return 0.0;\n",
    "            }\n",
    "\n",
    "            match groupwise.as_str() {\n",
    "                \"max\" => go1.par_iter()\n",
    "                    .map(|id1| go2.par_iter().map(|id2| sim_fn.compute(id1, id2, counter)).reduce(|| 0.0, f64::max))\n",
    "                    .reduce(|| 0.0, f64::max),\n",
    "                \"bma\" => {\n",
    "                    let sem1: Vec<_> = go1.par_iter()\n",
    "                        .map(|id1| go2.par_iter().map(|id2| sim_fn.compute(id1, id2, counter)).reduce(|| 0.0, f64::max))\n",
    "                        .collect();\n",
    "                    let sem2: Vec<_> = go2.par_iter()\n",
    "                        .map(|id2| go1.par_iter().map(|id1| sim_fn.compute(id1, id2, counter)).reduce(|| 0.0, f64::max))\n",
    "                        .collect();\n",
    "                    let total = sem1.len() + sem2.len();\n",
    "                    if total == 0 {\n",
    "                        0.0\n",
    "                    } else {\n",
    "                        (sem1.iter().sum::<f64>() + sem2.iter().sum::<f64>()) / total as f64\n",
    "                    }\n",
    "                }\n",
    "                _ => 0.0,\n",
    "            }\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    Ok(scores)\n",
    "}\n",
    "\n",
    "fn semantic_contributions(\n",
    "    go_id: &str,\n",
    "    terms: &HashMap<String, crate::go_ontology::GOTerm>,\n",
    ") -> HashMap<String, f64> {\n",
    "    // Static cache for memoization\n",
    "    static SEMANTIC_CONTRIB_CACHE: once_cell::sync::Lazy<DashMap<String, HashMap<String, f64>>> = once_cell::sync::Lazy::new(|| DashMap::new());\n",
    "\n",
    "    // Check cache first\n",
    "    if let Some(cached) = SEMANTIC_CONTRIB_CACHE.get(go_id) {\n",
    "        return cached.clone();\n",
    "    }\n",
    "\n",
    "    let mut contributions = HashMap::default();\n",
    "    let mut to_visit = vec![(go_id, 1.0)];\n",
    "\n",
    "    while let Some((current_id, weight)) = to_visit.pop() {\n",
    "        if weight < 1e-6 || contributions.contains_key(current_id) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        contributions.insert(current_id.to_string(), weight);\n",
    "\n",
    "        if let Some(term) = terms.get(current_id) {\n",
    "            // is_a → 0.8\n",
    "            for parent in &term.parents {\n",
    "                to_visit.push((parent, weight * 0.8));\n",
    "            }\n",
    "            // part_of → 0.6\n",
    "            for (rel_type, target) in &term.relationships {\n",
    "                let rel_weight = match rel_type.as_str() {\n",
    "                    \"part_of\" => 0.6,\n",
    "                    _ => continue,  // skip other relationships for now\n",
    "                };\n",
    "                to_visit.push((target, weight * rel_weight));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Store in cache as HashMap<String, f64>\n",
    "    SEMANTIC_CONTRIB_CACHE.insert(go_id.to_string(), contributions.clone());\n",
    "    contributions\n",
    "}\n",
    "\n",
    "// --- TopoICSim and helper functions ---\n",
    "\n",
    "/// Compute the Inverse Information Content (IIC) for a term.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// go_id : str\n",
    "///   GO term ID.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed term counter with IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   The IIC value for the term.\n",
    "fn iic(go_id: &str, counter: &TermCounter) -> f64 {\n",
    "    let ic = *counter.ic.get(go_id).unwrap_or(&0.0);\n",
    "    if ic > 0.0 {\n",
    "        1.0 / ic\n",
    "    } else {\n",
    "        // If IC is zero, treat as very large (effectively infinite path weight)\n",
    "        1e12\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Compute the weighted shortest path (sum of IICs) from source to target (ancestor) in the GO DAG.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// source : str\n",
    "///   Source GO term ID.\n",
    "/// target : str\n",
    "///   Target GO term ID (ancestor).\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed term counter with IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// Option<float>\n",
    "///   Minimum sum of IICs along any path from source to target, or None if not connected.\n",
    "fn weighted_shortest_path_iic(\n",
    "    source: &str,\n",
    "    target: &str,\n",
    "    terms: &HashMap<String, crate::go_ontology::GOTerm>,\n",
    "    counter: &TermCounter,\n",
    ") -> Option<f64> {\n",
    "    use std::collections::{BinaryHeap, HashMap};\n",
    "    use std::cmp::Ordering;\n",
    "    #[derive(Copy, Clone, PartialEq)]\n",
    "    struct State {\n",
    "        cost: f64,\n",
    "        node: usize,\n",
    "    }\n",
    "    impl Eq for State {}\n",
    "    impl Ord for State {\n",
    "        fn cmp(&self, other: &Self) -> Ordering {\n",
    "            // Reverse order for min-heap\n",
    "            other.cost.partial_cmp(&self.cost).unwrap_or(Ordering::Equal)\n",
    "        }\n",
    "    }\n",
    "    impl PartialOrd for State {\n",
    "        fn partial_cmp(&self, other: &Self) -> Option<Ordering> {\n",
    "            Some(self.cmp(other))\n",
    "        }\n",
    "    }\n",
    "    // Map node ids to indices for fast lookup\n",
    "    let mut id2idx = HashMap::new();\n",
    "    let mut idx2id = Vec::new();\n",
    "    for (i, id) in terms.keys().enumerate() {\n",
    "        id2idx.insert(id.as_str(), i);\n",
    "        idx2id.push(id.as_str());\n",
    "    }\n",
    "    let src = match id2idx.get(source) {\n",
    "        Some(&i) => i,\n",
    "        None => return None,\n",
    "    };\n",
    "    let tgt = match id2idx.get(target) {\n",
    "        Some(&i) => i,\n",
    "        None => return None,\n",
    "    };\n",
    "    let mut dist = vec![f64::INFINITY; idx2id.len()];\n",
    "    dist[src] = iic(source, counter);\n",
    "    let mut heap = BinaryHeap::new();\n",
    "    heap.push(State { cost: dist[src], node: src });\n",
    "    while let Some(State { cost, node }) = heap.pop() {\n",
    "        let node_id = idx2id[node];\n",
    "        if node == tgt {\n",
    "            return Some(cost);\n",
    "        }\n",
    "        if cost > dist[node] {\n",
    "            continue;\n",
    "        }\n",
    "        if let Some(term) = terms.get(node_id) {\n",
    "            for parent in &term.parents {\n",
    "                if let Some(&parent_idx) = id2idx.get(parent.as_str()) {\n",
    "                    let next = cost + iic(parent, counter);\n",
    "                    if next < dist[parent_idx] {\n",
    "                        dist[parent_idx] = next;\n",
    "                        heap.push(State { cost: next, node: parent_idx });\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    None\n",
    "}\n",
    "\n",
    "/// Compute the weighted longest path (sum of IICs) from source to target (ancestor) in the GO DAG.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// source : str\n",
    "///   Source GO term ID.\n",
    "/// target : str\n",
    "///   Target GO term ID (ancestor).\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed term counter with IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// Option<float>\n",
    "///   Maximum sum of IICs along any path from source to target, or None if not connected.\n",
    "fn weighted_longest_path_iic(\n",
    "    source: &str,\n",
    "    target: &str,\n",
    "    terms: &HashMap<String, crate::go_ontology::GOTerm>,\n",
    "    counter: &TermCounter,\n",
    ") -> Option<f64> {\n",
    "    // Memoization\n",
    "    use std::collections::HashMap as StdHashMap;\n",
    "    fn dfs(\n",
    "        node: &str,\n",
    "        target: &str,\n",
    "        terms: &HashMap<String, crate::go_ontology::GOTerm>,\n",
    "        counter: &TermCounter,\n",
    "        memo: &mut StdHashMap<String, Option<f64>>,\n",
    "    ) -> Option<f64> {\n",
    "        if node == target {\n",
    "            return Some(iic(node, counter));\n",
    "        }\n",
    "        if let Some(&val) = memo.get(node) {\n",
    "            return val;\n",
    "        }\n",
    "        let mut max_path = None;\n",
    "        if let Some(term) = terms.get(node) {\n",
    "            for parent in &term.parents {\n",
    "                if let Some(sub) = dfs(parent, target, terms, counter, memo) {\n",
    "                    let total = iic(node, counter) + sub;\n",
    "                    max_path = Some(max_path.map_or(total, |m: f64| m.max(total)));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        memo.insert(node.to_string(), max_path);\n",
    "        max_path\n",
    "    }\n",
    "    let mut memo = StdHashMap::new();\n",
    "    dfs(source, target, terms, counter, &mut memo)\n",
    "}\n",
    "\n",
    "/// Compute the Disjunctive Common Ancestor set for two terms (as per TopoICSim paper).\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// id1 : str\n",
    "///   First GO term ID.\n",
    "/// id2 : str\n",
    "///   Second GO term ID.\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of str\n",
    "///   List of disjunctive common ancestor GO term IDs.\n",
    "fn disjunctive_common_ancestors(\n",
    "    id1: &str,\n",
    "    id2: &str,\n",
    "    terms: &HashMap<String, crate::go_ontology::GOTerm>,\n",
    ") -> Vec<String> {\n",
    "    use std::collections::HashSet;\n",
    "    // 1. Get all common ancestors\n",
    "    let ancestors1 = crate::go_ontology::collect_ancestors(id1, terms);\n",
    "    let ancestors2 = crate::go_ontology::collect_ancestors(id2, terms);\n",
    "    let common: HashSet<_> = ancestors1.intersection(&ancestors2).cloned().collect();\n",
    "    // 2. For each x in common, check if no child of x is also in common\n",
    "    let mut dca = Vec::new();\n",
    "    for x in &common {\n",
    "        let is_disjunctive = terms.get(x).map_or(false, |term| {\n",
    "            term.children.iter().all(|c| !common.contains(c))\n",
    "        });\n",
    "        if is_disjunctive {\n",
    "            dca.push(x.clone());\n",
    "        }\n",
    "    }\n",
    "    dca\n",
    "}\n",
    "\n",
    "/// Find all root terms (terms with no parents) in the ontology.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of str\n",
    "///   List of root GO term IDs.\n",
    "fn find_roots(terms: &HashMap<String, crate::go_ontology::GOTerm>) -> Vec<String> {\n",
    "    terms.iter()\n",
    "        .filter(|(_, term)| term.parents.is_empty())\n",
    "        .map(|(id, _)| id.clone())\n",
    "        .collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732055e4",
   "metadata": {},
   "source": [
    "## ¿Por qué varían los resultados de R y GoSemSim?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

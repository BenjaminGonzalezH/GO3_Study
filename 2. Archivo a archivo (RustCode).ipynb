{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56853acc",
   "metadata": {},
   "source": [
    "# Archivo a archivo\n",
    "Este apartado tiene como propósito presentar el estudio detallado de cada archivo existente dentro del código Rust del paquete go3, con el objetivo de explicar su funcionamiento clave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5bfca",
   "metadata": {},
   "source": [
    "## Código 1 - libs.rs\n",
    "Este archivo actúa como una capa intermedia que permite la comunicación entre Python y Rust. Por lo tanto, se deduce que la implementación de las funcionalidades principales del paquete está desarrollada completamente en Rust, y luego es expuesta a Python mediante _PyO3_, el cual permite invocarlas utilizando la sintaxis del lenguaje de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142486f",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// En esta sección se realiza la envoltura de los archivos Rust con pyo3 estableciendolo en\n",
    "// 3 pasos:\n",
    "use pyo3::prelude::*;           //Rust comprende Python (viceversa).\n",
    "use pyo3::types::PyModule;      //Rust entiende el concepto de módulo de Python.\n",
    "use pyo3::wrap_pyfunction;      //Python invoca código de Rust.\n",
    "\n",
    "// Se solicita la carga de tres archivo existentes en el mismo directorio en el que se\n",
    "// encuentra el archivo actual.\n",
    "pub mod go_loader;              // Lectura de archivos de entrada, relaciones e IC.\n",
    "pub mod go_ontology;            // --\n",
    "pub mod go_semantic;            // --\n",
    "\n",
    "// Dentro de cada uno de estos archivos mencionados previamente, se solicita la disponibilidad\n",
    "// de las funciones que se mencionan a continuación.\n",
    "use go_loader::{load_go_terms, load_gaf, build_term_counter};\n",
    "use go_ontology::{get_term_by_id, ancestors, common_ancestor, deepest_common_ancestor};\n",
    "use go_semantic::{\n",
    "    term_ic,\n",
    "    semantic_similarity,\n",
    "    termset_similarity,\n",
    "    batch_similarity,\n",
    "    compare_genes,\n",
    "    compare_gene_pairs_batch,\n",
    "    gene_distance_matrix,\n",
    "    tsne_genes,\n",
    "    umap_genes,\n",
    "    plot_embedding,\n",
    "    plot_tsne_genes,\n",
    "    plot_umap_genes,\n",
    "    set_num_threads,\n",
    "};\n",
    "\n",
    "// En esta sección se realiza la envoltura de todas las componentes invocadas anteriormente\n",
    "// de forma en que estas sean reconocidad como modulo de Python.\n",
    "#[pymodule]\n",
    "fn go3(_py: Python, m: &Bound<'_, PyModule>) -> PyResult<()> {\n",
    "    m.add_function(wrap_pyfunction!(load_go_terms, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(load_gaf, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(build_term_counter, m)?)?;\n",
    "\n",
    "    m.add_function(wrap_pyfunction!(get_term_by_id, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(ancestors, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(common_ancestor, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(deepest_common_ancestor, m)?)?;\n",
    "\n",
    "    m.add_function(wrap_pyfunction!(set_num_threads, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(term_ic, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(semantic_similarity, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(termset_similarity, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(batch_similarity, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(compare_genes, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(compare_gene_pairs_batch, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(gene_distance_matrix, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(tsne_genes, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(umap_genes, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(plot_embedding, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(plot_tsne_genes, m)?)?;\n",
    "    m.add_function(wrap_pyfunction!(plot_umap_genes, m)?)?;\n",
    "\n",
    "    m.add_class::<go_ontology::PyGOTerm>()?;\n",
    "    m.add_class::<go_loader::GAFAnnotation>()?;\n",
    "    m.add_class::<go_loader::TermCounter>()?;\n",
    "\n",
    "    Ok(())                                                          // Indicador de exito en la exportación de funciones Rust a Python.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccaf8f",
   "metadata": {},
   "source": [
    "## Código 2 - go_loader.rs\n",
    "Archivo encargado del procesamiento de los archivos .obo y gaf para la construcción de las relaciones jerarquicas entre terminos, relaciones gen a termino y el cálculo de IC de cada término existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc783740",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// ---------------- Librerias.\n",
    "use pyo3::prelude::*;                                               // Rust comprende Python (viceversa).\n",
    "use once_cell::sync::OnceCell;                                      // Implementación de variables globales de una sola carga.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};       // Invocación de tablas hash.\n",
    "use std::io::{BufReader, BufRead};                                  // Lectura eficiente de archivos.\n",
    "use std::fs::File;                                                  // Apertura y manejo de archivos.\n",
    "use parking_lot::RwLock;                                            // Administración de hilos por semaforo.\n",
    "use std::path::Path;                                                // Administración de rutas.\n",
    "use std::fs;                                                        // Administración de entradas y salidas.\n",
    "use reqwest::blocking::get;                                         // Peticiones para obtención de recursos en internet.\n",
    "use rayon::prelude::*;                                              // Automatización del uso de todos los nucleos disponibles de un dispositivo.\n",
    "\n",
    "\n",
    "// ---------------- Componentes globales.\n",
    "// Declaración de entidades estáticas en memoria y global en toda la ejecución del código;\n",
    "// cabe señalar que estos son persistentes mientras el código se ejecuta y estos poseen la\n",
    "// capacidad de ser leidos por varios threads con bloqueo si se requiere escribir.\n",
    "use crate::go_ontology::{GOTerm, PyGOTerm, collect_ancestors, get_terms_or_error};\n",
    "pub static GO_TERMS_CACHE: OnceCell<RwLock<HashMap<String, GOTerm>>> = OnceCell::new();                 // Hash entre el GO_ID y su información.\n",
    "pub static GENE2GO_CACHE: OnceCell<RwLock<HashMap<String, Vec<String>>>> = OnceCell::new();             // Hash entre un gen y sus términos GO relacionados.\n",
    "pub static ANCESTORS_CACHE: OnceCell<RwLock<HashMap<String, HashSet<String>>>> = OnceCell::new();       // Hash entre un GO_ID y sus respectivos ancestros.\n",
    "pub static DCA_CACHE: OnceCell<RwLock<HashMap<(String, String), String>>> = OnceCell::new();            // DCA - Deep Common Ancestor entre un par de terminos.\n",
    "\n",
    "\n",
    "//---------------- Administración de cache.\n",
    "// Esta función administra las versiones del caché, por ende, permite siempre crear\n",
    "// una reserva de memoria inicial pero que puede actualizarse según corresponda.\n",
    "fn set_or_replace_cache<T>(cell: &OnceCell<RwLock<T>>, value: T) {\n",
    "    if let Some(lock) = cell.get() {\n",
    "        *lock.write() = value;\n",
    "    } else {\n",
    "        let _ = cell.set(RwLock::new(value));\n",
    "    }\n",
    "}\n",
    "\n",
    "//---------------- Estructuras.\n",
    "\n",
    "/** GAFAnnotation\n",
    "De este componente se puede comprender que solamente del GAF se obtiene el db_object_id\n",
    "el go_term y la evidencia asociada a este mismo, siendo las tres columnas claves.\n",
    "*/\n",
    "#[pyclass]                      // Se considera una clase compatible con Python.\n",
    "#[derive(Clone)]                // Permite replicación/copia si es requerido.\n",
    "pub struct GAFAnnotation {\n",
    "    #[pyo3(get)]\n",
    "    pub db_object_id: String,   // Obtención del identificador asociado al gen.\n",
    "    #[pyo3(get)]\n",
    "    pub go_term: String,        // GoTerm asociado al gen.\n",
    "    #[pyo3(get)]\n",
    "    pub evidence: String,       // Evidencia que respalda la asociación entre ambos.\n",
    "}\n",
    "\n",
    "/** TermCounter.\n",
    "Estructura encargada de establecer la computación de information content (IC) de\n",
    "cada termino GO asociado.\n",
    "*/\n",
    "#[pyclass]\n",
    "#[derive(Clone)]\n",
    "pub struct TermCounter {\n",
    "    #[pyo3(get)]\n",
    "    pub counts: HashMap<String, usize>,         // Contador de aparición de un térmiono dentro del .gaf\n",
    "    #[pyo3(get)]\n",
    "    pub total_by_ns: HashMap<String, usize>,    // Contador de términos únicos por ontología de GO.\n",
    "    #[pyo3(get)]\n",
    "    pub ic: HashMap<String, f64>,               // Information Content (IC) de cáda anotación.\n",
    "}\n",
    "\n",
    "//---------------- Funciones Implementadas.\n",
    "\n",
    "/** parse_obo (function)\n",
    "Función dedicada a la lectura de todos los términos existentes dentro de un archivo .obo para\n",
    "obtener su información que será guardada en caché, asimismo, se asegura de identificar los\n",
    "ancestros, computar profundidad-nivel dentro del DAG e identificar los ancestros asociados a\n",
    "cada uno.\n",
    "*/\n",
    "pub fn parse_obo(\n",
    "    path: &str                                                                                      // Cadena de caracteres del directorio con el .obo.\n",
    ") \n",
    "-> HashMap<String, GOTerm> {\n",
    "    let contents = fs::read_to_string(path).expect(\"Can't open OBO file\");                          // Carga completa en memoria del archivo\n",
    "    let chunks = contents.split(\"[Term]\");                                                          // Separación de chunks de líneas según campo [Term]\n",
    "\n",
    "    let canonical_terms: Vec<GOTerm> = chunks                                                       // Procesamiento paralelo de GoTerms (ignora obsoletos o mal formateados).\n",
    "        .par_bridge()\n",
    "        .filter_map(parse_term_chunk)\n",
    "        .filter(|term| !term.is_obsolete)\n",
    "        .collect();\n",
    "\n",
    "    let mut term_map: HashMap<String, GOTerm> =                                                     // Preparación de memoria y configurar uso de hashing rápido.\n",
    "        HashMap::with_capacity_and_hasher(canonical_terms.len() * 2, Default::default());\n",
    "\n",
    "    for term in canonical_terms.into_iter() {                                                       // Procesamiento de otros identificadores y términos asociados alternativos\n",
    "        let mut all_ids = term.alt_ids.clone();                                                     // a un GoTerm de forma en que todos \"apunten\" a la misma información en HASH.\n",
    "        all_ids.push(term.id.clone());\n",
    "\n",
    "        // Este proceso de clnanción lo que hace es que en cada id alternativo\n",
    "        // almacenado en HASH (procesado en línea anteriores) se le clone\n",
    "        // los datos del \"término principal\".\n",
    "        for id in &all_ids {\n",
    "            let mut clone = term.clone();\n",
    "            clone.id = id.clone();\n",
    "            clone.alt_ids = all_ids.clone();\n",
    "            term_map.insert(id.clone(), clone);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Se tiene que recordar que -\n",
    "    // Nivel: Distancia mínima desde la \"raíz\" hacia un nodo.\n",
    "    // Profundidad: Distancia máxima desde la \"raíz\" hacia un nodo..\n",
    "    compute_levels_and_depths(&mut term_map);                                                       // Se Calcula profundidad/especifidad de cada término.\n",
    "\n",
    "    // Función dedicada a la recolección de ancestro de un GOTerm\n",
    "    // en particular y que este no haya sido identificado previamente.\n",
    "    fn collect_ancestors_uncached(\n",
    "        go_id: &str,                                                                                // Identificador de GoTerms. \n",
    "        terms: &HashMap<String, GOTerm>                                                             // Todos los términos mapeados del .obo.\n",
    "    ) -> HashSet<String> {\n",
    "        let mut visited = HashSet::default();                                                       // Conjunto de visitados.\n",
    "        let mut stack = vec![go_id];                                                                // Pila de GoTerms a procesar (inicia con el proporcionado).\n",
    "        while let Some(current) = stack.pop() {\n",
    "            if visited.insert(current.to_string()) {                                                // Por cada GoTerm procesado este se señala como visitado.\n",
    "                if let Some(term) = terms.get(current) {\n",
    "                    for parent in &term.parents {                                                   // Agrega a la Pila los padres del término procesado (ancestros).\n",
    "                        stack.push(parent);\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        visited                                                                                     // Se devuelven los ancestros.\n",
    "    }\n",
    "\n",
    "    let ancestors_map: HashMap<String, HashSet<String>> = term_map                                  // Se identifican los ancestros a cada GoTerm y se dejan en caché.\n",
    "        .par_iter()\n",
    "        .map(|(id, _)| {\n",
    "            let ancestors = crate::go_ontology::collect_ancestors(id, &term_map)\n",
    "                .into_iter()\n",
    "                .map(|s| s.to_string())\n",
    "                .collect();\n",
    "            (id.clone(), ancestors)\n",
    "        })\n",
    "        .collect();\n",
    "    set_or_replace_cache(&ANCESTORS_CACHE, ancestors_map);;                                        // Guardado en cache.\n",
    "    set_or_replace_cache(&DCA_CACHE, HashMap::default());                                          // Se inicializa cache de ancestro más cercano (se calcula por demanda).\n",
    "    crate::go_semantic::clear_internal_caches();                                                   // \n",
    "\n",
    "    term_map                                                                                       // Se retorna mapeo de GoTermns.\n",
    "}\n",
    "\n",
    "\n",
    "/** parse_term_chunk (function)\n",
    "Función encargada de procesar los campos asociados al texto (usual y opcional) que se encuentra\n",
    "en el archivo .obo existente en la computadora.\n",
    "*/\n",
    "fn parse_term_chunk(\n",
    "    chunk: &str                                                         // Líneas de texto definidas según una clave o sección repetida.\n",
    ") -> Option<GOTerm> {\n",
    "    // Definición de la estructura de un término (dentro del archivo).\n",
    "    let mut term = GOTerm {\n",
    "        id: String::new(),                                              // Identificador.\n",
    "        name: String::new(),                                            // Nombre.\n",
    "        namespace: String::new(),                                       // Ontología BP, MC, CC.\n",
    "        definition: String::new(),                                      // Definición.\n",
    "        parents: Vec::new(),                                            // Padres.\n",
    "        is_obsolete: false,                                             // ¿Es obsoleto?\n",
    "        alt_ids: Vec::new(),                                            // Ids alternativos.\n",
    "        replaced_by: None,                                              // Reemplazado por... (si esta obsoleto).\n",
    "        consider: Vec::new(),                                           // Considerar ¿Qué cosa?.\n",
    "        synonyms: Vec::new(),                                           // Sinónimos.\n",
    "        xrefs: Vec::new(),                                              // Referencias asociadas a...\n",
    "        relationships: Vec::new(),                                      // Relaciones con otros términois.\n",
    "        comment: None,                                                  // Comentarios.\n",
    "        children: Vec::new(),                                           // Hijos.\n",
    "        level: None,                                                    // Nivel.\n",
    "        depth: None,                                                    // Profundidad/Especificidad.\n",
    "    };\n",
    "\n",
    "    let chunk = chunk.split(\"[Typedef]\").next().unwrap_or(chunk);       // Ignorar campos Typedef.\n",
    "\n",
    "    let lines: Vec<&str> = chunk\n",
    "        .lines()\n",
    "        .map(|l| l.trim())                                              // Eliminamos espacios a izquierda y derecha.\n",
    "        .filter(|l| !l.is_empty())                                      // Quitamos líneas vacías.\n",
    "        .collect();\n",
    "\n",
    "    if lines.is_empty() {                                               // Si no se encuentra texto se finaliza la función.                                           \n",
    "        return None;\n",
    "    }\n",
    "    let mut valid = false;\n",
    "\n",
    "    // Verificación de campos que se obtienen de cada sección existente en Term.\n",
    "    for line in lines {\n",
    "        if line.starts_with(\"id: \") {                                               // Verificación para que sea válido (debe tener campo ID).\n",
    "            term.id = line[\"id: \".len()..].to_string();\n",
    "            valid = true;\n",
    "        } else if line.starts_with(\"name: \") {                                      // Verificación del campo de nombre.\n",
    "            term.name = line[\"name: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"namespace: \") {                                 // Verificación del campo namespace.\n",
    "            term.namespace = line[\"namespace: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"def: \") {                                       // Verificación de definición.\n",
    "            term.definition = line[\"def: \".len()..].to_string();\n",
    "        } else if line.starts_with(\"is_a: \") {                                      // Obtención de lisa de padres.\n",
    "            let parent = line[\"is_a: \".len()..]\n",
    "                .split_whitespace()\n",
    "                .next()\n",
    "                .unwrap_or(\"\")\n",
    "                .to_string();\n",
    "            if !parent.is_empty() {\n",
    "                term.parents.push(parent);\n",
    "            }\n",
    "        } else if line.starts_with(\"alt_id: \") {                                    // Verificar términos alternativos.\n",
    "            term.alt_ids.push(line[\"alt_id: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"is_obsolete: true\") {                           // Verificar obsolencia.\n",
    "            term.is_obsolete = true;\n",
    "        } else if line.starts_with(\"replaced_by: \") {                               // Obtener id de reemplazo.\n",
    "            term.replaced_by = Some(line[\"replaced_by: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"consider: \") {                                  // Obtener campo de consideración.\n",
    "            term.consider.push(line[\"consider: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"synonym: \") {                                   // Obtener campo de sinónimo.\n",
    "            term.synonyms.push(line[\"synonym: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"xref: \") {                                      // Obtener campo de xref.\n",
    "            term.xrefs.push(line[\"xref: \".len()..].to_string());\n",
    "        } else if line.starts_with(\"relationship: \") {                              // Obtener campo de relación..\n",
    "            let rel_def = &line[\"relationship: \".len()..];\n",
    "            let mut parts = rel_def.split_whitespace();\n",
    "            if let (Some(rel), Some(target)) = (parts.next(), parts.next()) {\n",
    "                term.relationships.push((rel.to_string(), target.to_string()));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if valid {\n",
    "        Some(term)                                                                  // Si el termino es válido es añadido, en caso contrario se ignora.\n",
    "    } else {\n",
    "        None\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "/** compute_levels_and_depths (function)\n",
    "Función encargada de cálcular el nivel y profundidad de cada GoTerm asociado.\n",
    "*/\n",
    "pub fn compute_levels_and_depths(\n",
    "    terms: &mut HashMap<String, GOTerm>                                 // Yérminos mepados.\n",
    ") {\n",
    "    \n",
    "    // Paso 1: construir mapa de hijos teniendo en consideración las relaciones\n",
    "    // is_a para establecer jerarquía padre-hijo.\n",
    "    let mut child_map: HashMap<String, Vec<String>> = HashMap::default();\n",
    "    for (id, term) in terms.iter() {\n",
    "        for parent in &term.parents {\n",
    "            child_map.entry(parent.clone()).or_default().push(id.clone());\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Paso 2: inicializar level como una componente que identifica la distancia mínima\n",
    "    // entre el término raiz y el termino en estudio. Esto como función recursiva interna\n",
    "    // que selecciona camino según los padres de menor distancia.\n",
    "    fn init_level(\n",
    "        term_id: &str,                                                  // Identificador GoTerm.\n",
    "        terms: &mut HashMap<String, GOTerm>,                            // Mapa de términos.\n",
    "        visiting: &mut HashSet<String>,                                 // Nodos que se recorren.\n",
    "    ) -> usize {\n",
    "        if visiting.contains(term_id) {\n",
    "            // Ciclo detectado: se evita recursión infinita.\n",
    "            eprintln!(\"⚠️ Ciclo detectado en level: {}\", term_id);\n",
    "            return 0;\n",
    "        }\n",
    "\n",
    "        // Caso base: Si el nivel ya fue cálculado, este se devuelve\n",
    "        // se optimiza el cálculo.\n",
    "        if let Some(level) = terms.get(term_id).and_then(|t| t.level) {\n",
    "            return level;\n",
    "        }\n",
    "\n",
    "        // Seguimiento de nodos que se visitan.\n",
    "        visiting.insert(term_id.to_string());\n",
    "\n",
    "        // Se obtienen padres directos.\n",
    "        let parents = terms\n",
    "            .get(term_id)\n",
    "            .map(|t| t.parents.clone())\n",
    "            .unwrap_or_default();\n",
    "\n",
    "        // Si no hay se considera el nivel 0 raíz, en caso contrario\n",
    "        // es llamada recursiva para seguir buscando la raíz.\n",
    "        let level = if parents.is_empty() {\n",
    "            0\n",
    "        } else {\n",
    "            parents\n",
    "                .iter()\n",
    "                .map(|p| init_level(p, terms, visiting))\n",
    "                .min()\n",
    "                .unwrap_or(0) + 1\n",
    "        };\n",
    "\n",
    "        // Se quita term_id de los que se están procesando para evitar\n",
    "        // evitar otra llamada innecesaria.\n",
    "        visiting.remove(term_id);\n",
    "        if let Some(term) = terms.get_mut(term_id) {\n",
    "            term.level = Some(level);\n",
    "        }\n",
    "\n",
    "        // Se devuelve el nivel.\n",
    "        level\n",
    "    }\n",
    "\n",
    "    // Paso 3: inicializar depth como la distancia más larga a la raíz. Mismo concepto \n",
    "    // que punto anterior.\n",
    "    fn init_depth(\n",
    "        term_id: &str,\n",
    "        terms: &mut HashMap<String, GOTerm>,\n",
    "        visiting: &mut HashSet<String>,\n",
    "    ) -> usize {\n",
    "        if visiting.contains(term_id) {\n",
    "            eprintln!(\"Ciclo detectado en depth: {}\", term_id);\n",
    "            return 0;\n",
    "        }\n",
    "\n",
    "        if let Some(depth) = terms.get(term_id).and_then(|t| t.depth) {\n",
    "            return depth;\n",
    "        }\n",
    "\n",
    "        visiting.insert(term_id.to_string());\n",
    "\n",
    "        let parents = terms\n",
    "            .get(term_id)\n",
    "            .map(|t| t.parents.clone())\n",
    "            .unwrap_or_default();\n",
    "\n",
    "        let depth = if parents.is_empty() {\n",
    "            0\n",
    "        } else {\n",
    "            parents\n",
    "                .iter()\n",
    "                .map(|p| init_depth(p, terms, visiting))\n",
    "                .max()\n",
    "                .unwrap_or(0) + 1\n",
    "        };\n",
    "\n",
    "        visiting.remove(term_id);\n",
    "        if let Some(term) = terms.get_mut(term_id) {\n",
    "            term.depth = Some(depth);\n",
    "        }\n",
    "\n",
    "        depth\n",
    "    }\n",
    "\n",
    "    // Paso 4: recorrer todos los términos y calcular level + depth como llamados\n",
    "    // a las funciones por cada ids.\n",
    "    let ids: Vec<String> = terms.keys().cloned().collect();\n",
    "    for id in &ids {\n",
    "        let mut visiting = HashSet::default();\n",
    "        init_level(id, terms, &mut visiting);\n",
    "\n",
    "        let mut visiting = HashSet::default();\n",
    "        init_depth(id, terms, &mut visiting);\n",
    "    }\n",
    "\n",
    "    // Paso 5: rellenar el campo children con los hijos (solo vía is_a).\n",
    "    for (parent, children) in child_map {\n",
    "        if let Some(term) = terms.get_mut(&parent) {\n",
    "            term.children = children;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "/** download_obo (function)\n",
    "Obtención de archivo .obo verificando si este existe en la ruta de destino, por defecto,\n",
    "descarga la versión básica. */\n",
    "pub fn download_obo() -> Result<String, String> {\n",
    "    let obo_path = \"go-basic.obo\";                                      // Buscar en el directorio actual el archivo\n",
    "    if Path::new(obo_path).exists() {\n",
    "        return Ok(obo_path.to_string());\n",
    "    }\n",
    "\n",
    "    let url = \"http://purl.obolibrary.org/obo/go/go-basic.obo\";         // En caso de no encontrarlo se descarga.\n",
    "    println!(\"Descargando ontología desde: {}\", url);\n",
    "    let response = get(url).map_err(|e| e.to_string())?;\n",
    "\n",
    "    let content = response.text().map_err(|e| e.to_string())?;\n",
    "    fs::write(obo_path, content).map_err(|e| e.to_string())?;\n",
    "\n",
    "    Ok(obo_path.to_string())                                            // Se devuelve como string para procesarlo.\n",
    "}\n",
    "\n",
    "\n",
    "/** load_go_terms (function).\n",
    "En esta función se realiza el llamado al parseo de GoTerms y su guardado en chache\n",
    "para no necesitar re-ejecuciones de la función.\n",
    "*/\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (path=None))]                                            // Valor por defecto en Python = None.\n",
    "pub fn load_go_terms(\n",
    "    path: Option<String>                                                    // Directorio de archivo .obo.\n",
    ") -> PyResult<Vec<PyGOTerm>> {\n",
    "    let path = match path {                                                 // En caso de no encontrar archivo .obo lo descarga.\n",
    "        Some(p) => p,\n",
    "        None => download_obo()\n",
    "            .map_err(|e| pyo3::exceptions::PyIOError::new_err(e))?,\n",
    "    };\n",
    "    if !Path::new(&path).exists() {                                         // Si el directorio es incorrecto se informa.\n",
    "        return Err(pyo3::exceptions::PyIOError::new_err(format!(\n",
    "            \"OBO file not found: {}\",\n",
    "            path\n",
    "        )));\n",
    "    }\n",
    "    let terms_map = parse_obo(&path);                                      // Llamado a función parse_obo.\n",
    "\n",
    "    set_or_replace_cache(&GO_TERMS_CACHE, terms_map.clone());              // Guardar en la caché global.      \n",
    "    let terms_vec = terms_map                                              // Devolver lista de PyGOTerm.\n",
    "        .into_iter()\n",
    "        .map(|(_, v)| PyGOTerm::from(&v))\n",
    "        .collect();\n",
    "\n",
    "    Ok(terms_vec)\n",
    "}\n",
    "\n",
    "\n",
    "/** load_gaf (function)\n",
    "Función encargada de realizar el mapeo de genes con sus respectivos términos\n",
    "asociados según la información existente en el .gaf\n",
    "*/ \n",
    "#[pyfunction]\n",
    "pub fn load_gaf(\n",
    "    path: String                                                            // Directorio en donde se encuantra el archivo .gaf\n",
    ") -> PyResult<Vec<GAFAnnotation>> {\n",
    "    // Se verifica acceso al archivo .gaf existente, en caso de que este no pueda ser \n",
    "    // accedido, entonces la función termina.\n",
    "    let file = File::open(&path)\n",
    "        .map_err(|e| pyo3::exceptions::PyIOError::new_err(e.to_string()))?;\n",
    "    let reader = BufReader::new(file);\n",
    "\n",
    "    // Revisar GOTerms obsoletos dentro de lo cargado en caché.\n",
    "    // Recordar: Debieron haberse cargado en cache los go terms en primer lugar.\n",
    "    let terms = match crate::go_ontology::get_terms_or_error() {\n",
    "        Ok(t) => t,\n",
    "        Err(e) => return Err(e),\n",
    "    };\n",
    "\n",
    "    let mut annotations: Vec<GAFAnnotation> = Vec::new();                   // Vector de anotaciones de GAF (parte vacía).\n",
    "    let mut gene2go: HashMap<String, Vec<String>> = HashMap::default();     // Relación de genes a términos.\n",
    "\n",
    "    // Lectura linea a linea filtrando aquellas que no empiecen con '!'.\n",
    "    for line in reader.lines().filter_map(Result::ok).filter(|l| !l.starts_with('!')) {\n",
    "        let cols: Vec<&str> = line.split('\\t').collect();                   // Identificar columnas mediante una separación por tabuladores (son 7).\n",
    "        if cols.len() < 7 {                                                 // Debe tener como mínimo 7 campos como mínimo.\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // Columnas consideradas de cada linea válida.\n",
    "        let db_object_id = cols[1].to_string();\n",
    "        let qualifier = cols[3].to_string();\n",
    "        let mut go_term = cols[4].to_string();\n",
    "        let evidence = cols[6].to_string();\n",
    "        let gene = cols[2].to_string();\n",
    "\n",
    "        // Se ignoran registros de genes con terminos sin evidencia sustentada,\n",
    "        // asimismo de asociaciones del tipo \"El gen NO hace esto\".\n",
    "        // Filter out ND annotations.\n",
    "        if evidence == \"ND\" {\n",
    "            continue;\n",
    "        }\n",
    "        // Saltar NOT annotations.\n",
    "        if qualifier.contains(\"NOT\") {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "\n",
    "        // Resolver relación entre genes y términos obsoletos.\n",
    "        if let Some(term) = terms.get(&go_term) {\n",
    "            if term.is_obsolete {\n",
    "                if let Some(ref replacement) = term.replaced_by {\n",
    "                    // 1. Usar el término de reemplazo.\n",
    "                    go_term = replacement.clone();\n",
    "                } else if !term.consider.is_empty() {\n",
    "                    // 2. Si no funcionó el anterior, utilizar el primer término en \"Considerar\"\n",
    "                    go_term = term.consider[0].clone();\n",
    "                } else {\n",
    "                    // Si no hay término de reemplazo, omitir anotación.\n",
    "                    continue;\n",
    "                }\n",
    "            }\n",
    "        } else {\n",
    "            // GO term no encontrado.\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // Añadir anotación dentro de la estructura definida.\n",
    "        annotations.push(GAFAnnotation {\n",
    "            db_object_id: db_object_id.clone(),\n",
    "            go_term: go_term.clone(),\n",
    "            evidence,\n",
    "        });\n",
    "\n",
    "        // Hacer actualización de la estructura a cachear.\n",
    "        gene2go.entry(gene).or_default().push(go_term);\n",
    "    }\n",
    "\n",
    "    // Guardar en caché global.\n",
    "    let _ = GENE2GO_CACHE.set(RwLock::new(gene2go));\n",
    "\n",
    "    // Se entregan anotaciones como retorno.\n",
    "    Ok(annotations)\n",
    "}\n",
    "\n",
    "/** build_term_counter (function)\n",
    "Función encargada de ser una encapsuladora de la función interna que se\n",
    "dedica a cálcular el IC de cada término que se posea utilizando de referencia\n",
    "las anotaciones obtenidas del .gaf.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn build_term_counter(\n",
    "    py: Python<'_>,                                                         // Permisis para utilizar estructuras de Python.\n",
    "    py_annotations: Vec<Py<GAFAnnotation>>,                                 // Vector de anotaciones existente en python.\n",
    ") -> PyResult<TermCounter> {\n",
    "    // Obtener los términos GO desde el caché global.\n",
    "    let terms = get_terms_or_error()?;\n",
    "\n",
    "    // Convertir las anotaciones de Py<GAFAnnotation> a GAFAnnotation (Rust).\n",
    "    let annotations: Vec<GAFAnnotation> = py_annotations\n",
    "        .into_iter()\n",
    "        .map(|py_ann| py_ann.extract(py))\n",
    "        .collect::<PyResult<_>>()?;\n",
    "\n",
    "    // Llamar a la función de conteo interna.\n",
    "    Ok(_build_term_counter(&annotations, &terms))\n",
    "}\n",
    "\n",
    "/** _build_term_counter (function)\n",
    "Funci+on encargada de calcular el IC de cada GOTerm obtenido.\n",
    "*/\n",
    "fn _build_term_counter(\n",
    "    annotations: &[GAFAnnotation],                                          // Anotaciones del .gaf\n",
    "    terms: &HashMap<String, GOTerm>,                                        // Términos go de .obo\n",
    ") -> TermCounter {\n",
    "    // Se activa paralelismo para la lectura de la cache de ancestro y utilizando una variable\n",
    "    // de referencia para usarlo como elemento protegido.\n",
    "    let ancestors_cache_guard = ANCESTORS_CACHE.get().map(|lock| lock.read());\n",
    "    let ancestors_cache = ancestors_cache_guard.as_deref();\n",
    "\n",
    "    // Se construye con paralelismo el mapa que relaciona los genes con su término\n",
    "    // go y los ancestro de este mismo.\n",
    "    let obj_to_terms: HashMap<&str, HashSet<String>> = annotations\n",
    "        .par_iter()\n",
    "        .fold(HashMap::<&str, HashSet<String>>::default, |mut acc, ann| {\n",
    "            let go_id = ann.go_term.as_str();                                   // Se obtiene go_id de la anotación.\n",
    "            let entry = acc.entry(ann.db_object_id.as_str()).or_default();      // Se obtiene identificador de gen.\n",
    "\n",
    "            // Aquí se añade el término mismo con sus ancestro a la relación\n",
    "            // con el gen obtenido.\n",
    "            entry.insert(go_id.to_string());\n",
    "            if let Some(cache) = ancestors_cache {\n",
    "                if let Some(ancestors) = cache.get(go_id) {\n",
    "                    entry.extend(ancestors.iter().cloned());\n",
    "                } else {\n",
    "                    // Should be rare (unknown GO ID), but keep a correct fallback.\n",
    "                    entry.extend(collect_ancestors(go_id, terms).into_iter());\n",
    "                }\n",
    "            } else {\n",
    "                entry.extend(collect_ancestors(go_id, terms).into_iter());\n",
    "            }\n",
    "            acc\n",
    "        })\n",
    "        .reduce(HashMap::<&str, HashSet<String>>::default, |mut acc, map| {     // Aquí se unen los resultados de los núcleos.\n",
    "            for (k, v) in map {\n",
    "                acc.entry(k).or_default().extend(v);\n",
    "            }\n",
    "            acc\n",
    "        });\n",
    "\n",
    "    // Se construye el conteo de genes anotados en cáda GoTerm y términos\n",
    "    // únicos en cada ontología de GO.\n",
    "    let (counts, total_by_ns) = obj_to_terms\n",
    "        .par_iter()\n",
    "        .fold(\n",
    "            || (\n",
    "                HashMap::<String, usize>::default(),\n",
    "                HashMap::<String, usize>::default(),\n",
    "            ),\n",
    "            |(mut counts, mut total_by_ns), (_gene, term_ids)| {\n",
    "                let mut namespaces_seen: HashSet<&str> = HashSet::default();\n",
    "                for term_id in term_ids {\n",
    "                    if let Some(term) = terms.get(term_id.as_str()) {\n",
    "                        *counts.entry(term_id.clone()).or_insert(0) += 1;\n",
    "                        namespaces_seen.insert(term.namespace.as_str());\n",
    "                    }\n",
    "                }\n",
    "                for ns in namespaces_seen {\n",
    "                    *total_by_ns.entry(ns.to_string()).or_insert(0) += 1;\n",
    "                }\n",
    "                (counts, total_by_ns)\n",
    "            },\n",
    "        )\n",
    "        .reduce(\n",
    "            || (\n",
    "                HashMap::<String, usize>::default(),\n",
    "                HashMap::<String, usize>::default(),\n",
    "            ),\n",
    "            |(mut counts_a, mut total_a), (counts_b, total_b)| {\n",
    "                for (k, v) in counts_b {\n",
    "                    *counts_a.entry(k).or_insert(0) += v;\n",
    "                }\n",
    "                for (k, v) in total_b {\n",
    "                    *total_a.entry(k).or_insert(0) += v;\n",
    "                }\n",
    "                (counts_a, total_a)\n",
    "            },\n",
    "        );\n",
    "\n",
    "    // Cálculo final del IC: En caso de más información mire\n",
    "    // la fórmula en la documentación oficial.\n",
    "    let mut ic: HashMap<String, f64> = HashMap::default();\n",
    "    for (term_id, count) in &counts {\n",
    "        if let Some(term) = terms.get(term_id.as_str()) {\n",
    "            let total = total_by_ns.get(&term.namespace).copied().unwrap_or(1);\n",
    "            let freq = *count as f64 / total as f64;\n",
    "            let info_content = if freq > 0.0 { -freq.ln() } else { 0.0 };\n",
    "            ic.insert(term_id.clone(), info_content);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    TermCounter {\n",
    "        counts,\n",
    "        total_by_ns,\n",
    "        ic,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c3630",
   "metadata": {},
   "source": [
    "## Código 3 - go_ontology.rs\n",
    "Archivo encargado de la definición de la estructura utilizada para almacenar la información presente en los archivos .obo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18180c6c",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// ---------------- Librerias.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};       // Invocación de tablas hash.      \n",
    "use pyo3::prelude::*;                                               // Rust comprende Python (viceversa).\n",
    "use pyo3::types::PyString;                                          // Se invoca tipo de dato string de Python.\n",
    "use crate::go_loader::{GO_TERMS_CACHE, GENE2GO_CACHE};              // Se obtiene la cache generada en go_loader (si existe).\n",
    "use pyo3::exceptions::PyValueError;                                 // Se invoca 'error por valor' de Python.\n",
    "\n",
    "\n",
    "// ---------------- Estructuras.\n",
    "\n",
    "/** GoTerm\n",
    "Estructura enfocada en representar la información de\n",
    "un término GO.\n",
    "*/\n",
    "#[derive(Clone)]\n",
    "pub struct GOTerm {\n",
    "    pub id: String,                                     // Identificador del GOTerm.\n",
    "    pub name: String,                                   // Nombre del término.\n",
    "    pub namespace: String,                              // Ontología del término.\n",
    "    pub definition: String,                             // Definición.\n",
    "    pub parents: Vec<String>,                           // Identificadores padres del término.\n",
    "    pub children: Vec<String>,                          // Identificadores de los términos hijos.\n",
    "    pub depth: Option<usize>,                           // Profundidad [Opcional].\n",
    "    pub level: Option<usize>,                           // Nivel [Opcional].\n",
    "    pub is_obsolete: bool,                              // ¿Es obsoleto?\n",
    "    pub alt_ids: Vec<String>,                           // Identificadores de términos alternativos.\n",
    "    pub replaced_by: Option<String>,                    // En caso de ser obsoleto ¿Hay id's alternativos? [Opcional]\n",
    "    pub consider: Vec<String>,                          // Otros términos a considerar.\n",
    "    pub synonyms: Vec<String>,                          // Sinónimos asociados al término.\n",
    "    pub xrefs: Vec<String>,                             // Pendiente.\n",
    "    pub relationships: Vec<(String, String)>,           // Relaciones.\n",
    "    pub comment: Option<String>,                        // Comentarios [Opcional].\n",
    "}\n",
    "\n",
    "\n",
    "/** GoTerm\n",
    "Estructura enfocada en representar la información de\n",
    "un término GO, pero en su versión expuesta a Python.\n",
    "*/\n",
    "#[pyclass]\n",
    "#[derive(Clone)]\n",
    "pub struct PyGOTerm {\n",
    "    #[pyo3(get)] pub id: String,\n",
    "    #[pyo3(get)] pub name: String,\n",
    "    #[pyo3(get)] pub namespace: String,\n",
    "    #[pyo3(get)] pub definition: String,\n",
    "    #[pyo3(get)] pub parents: Vec<String>,\n",
    "    #[pyo3(get)] pub children: Vec<String>,\n",
    "    #[pyo3(get)] pub depth: Option<usize>,\n",
    "    #[pyo3(get)] pub level: Option<usize>,\n",
    "    #[pyo3(get)] pub is_obsolete: bool,\n",
    "    #[pyo3(get)] pub alt_ids: Vec<String>,\n",
    "    #[pyo3(get)] pub replaced_by: Option<String>,\n",
    "    #[pyo3(get)] pub consider: Vec<String>,\n",
    "    #[pyo3(get)] pub synonyms: Vec<String>,\n",
    "    #[pyo3(get)] pub xrefs: Vec<String>,\n",
    "    #[pyo3(get)] pub relationships: Vec<(String, String)>,\n",
    "    #[pyo3(get)] pub comment: Option<String>,\n",
    "}\n",
    "\n",
    "// ---------------- Configuraciones PyGoTerm.\n",
    "\n",
    "// Esta sección de código define como se debe realizar la copia de los\n",
    "// datos de la estructura GoTerm de Rust a su versión de Python de forma\n",
    "// en que no existan errores de compatibilidad.\n",
    "impl From<&GOTerm> for PyGOTerm {\n",
    "    fn from(term: &GOTerm) -> Self {\n",
    "        Self {\n",
    "            id: term.id.clone(),\n",
    "            name: term.name.clone(),\n",
    "            namespace: term.namespace.clone(),\n",
    "            definition: term.definition.clone(),\n",
    "            parents: term.parents.clone(),\n",
    "            children: term.children.clone(),\n",
    "            depth: term.depth,\n",
    "            level: term.level,\n",
    "            is_obsolete: term.is_obsolete,\n",
    "            alt_ids: term.alt_ids.clone(),\n",
    "            replaced_by: term.replaced_by.clone(),\n",
    "            consider: term.consider.clone(),\n",
    "            synonyms: term.synonyms.clone(),\n",
    "            xrefs: term.xrefs.clone(),\n",
    "            relationships: term.relationships.clone(),\n",
    "            comment: term.comment.clone(),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Esta sección de código define el comportamiento de Python a imprimir\n",
    "// un término GO como un mensaje que considera los elementos definidos\n",
    "// previamente. Además de automatizar la escritura del nombre del\n",
    "// objeto.\n",
    "#[pymethods]\n",
    "impl PyGOTerm {\n",
    "    fn __repr__(slf: &Bound<'_, Self>) -> PyResult<String> {\n",
    "        let class_name: Bound<'_, PyString> = slf.get_type().qualname()?;\n",
    "        let s = slf.borrow();\n",
    "        Ok(format!(\n",
    "            \"{} id: {}\\nname: {}\\nnamespace: {}\\ndefinition: {}\\nparents: {:?}\\nchildren: {:?}\\ndepth: {:?}\\nlevel: {:?}\\nis_obsolete: {}\\nalt_ids: {:?}\\nreplaced_by: {:?}\\nconsider: {:?}\\nsynonyms: {:?}\\nxrefs: {:?}\\nrelationships: {:?}\\ncomments: {:?}\",\n",
    "            class_name, s.id, s.name, s.namespace, s.definition, s.parents, s.children, s.depth, s.level,\n",
    "            s.is_obsolete, s.alt_ids, s.replaced_by, s.consider, s.synonyms, s.xrefs, s.relationships, s.comment\n",
    "        ))\n",
    "    }\n",
    "}\n",
    "\n",
    "// Esta sección genera un ciclo de regreso de la versión Python a la versión\n",
    "// de Rust, esto hace que cualquier modificación de una capa afecte a otra.\n",
    "impl From<PyGOTerm> for GOTerm {\n",
    "    fn from(py_term: PyGOTerm) -> Self {\n",
    "        Self {\n",
    "            id: py_term.id,\n",
    "            name: py_term.name,\n",
    "            namespace: py_term.namespace,\n",
    "            definition: py_term.definition,\n",
    "            parents: py_term.parents,\n",
    "            children: py_term.children,\n",
    "            depth: py_term.depth,\n",
    "            level: py_term.level,\n",
    "            is_obsolete: py_term.is_obsolete,\n",
    "            alt_ids: py_term.alt_ids,\n",
    "            replaced_by: py_term.replaced_by,\n",
    "            consider: py_term.consider,\n",
    "            synonyms: py_term.synonyms,\n",
    "            xrefs: py_term.xrefs,\n",
    "            relationships: py_term.relationships,\n",
    "            comment: py_term.comment,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ---------------- Estructuras.\n",
    "\n",
    "/** get_terms_or_error (function)\n",
    "Obtiene de la cache los datos de los GoTerms cargados, en caso\n",
    "de no existir se arroja error.\n",
    "*/\n",
    "pub fn get_terms_or_error<'a>() -> PyResult<parking_lot::RwLockReadGuard<'a, HashMap<String, GOTerm>>> {\n",
    "    Ok(\n",
    "        GO_TERMS_CACHE\n",
    "            .get()\n",
    "            .ok_or_else(|| pyo3::exceptions::PyRuntimeError::new_err(\"GO terms not loaded. Call go3.load_go_terms() first.\"))?\n",
    "            .read()\n",
    "    )\n",
    "}\n",
    "\n",
    "/** get_gene2go_or_error (function)\n",
    "Obtiene de la cache los datos del mapeo que relaciona genes con sus términso GO \n",
    "asociados, en caso de no existir se arroja error.\n",
    "*/\n",
    "pub fn get_gene2go_or_error<'a>() -> PyResult<parking_lot::RwLockReadGuard<'a, HashMap<String, Vec<String>>>> {\n",
    "    Ok(\n",
    "        GENE2GO_CACHE\n",
    "            .get()\n",
    "            .ok_or_else(|| pyo3::exceptions::PyRuntimeError::new_err(\"Gene2GO mapping not loaded. Call go3.load_gene2go() first.\"))?\n",
    "            .read()\n",
    "    )\n",
    "}\n",
    "\n",
    "/** get_term_by_id (function)\n",
    "Obtiene un GOTerm según el id proporcionado.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn get_term_by_id(\n",
    "    go_id: &str                             // Identificador de GoTerms.\n",
    ") -> PyResult<PyGOTerm> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    match terms.get(go_id) {\n",
    "        Some(term) => Ok(PyGOTerm::from(term)),\n",
    "        None => Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id\n",
    "        ))),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "/** collect_ancestors (function)\n",
    "Obtiene los ancestros asociados a un GoTerm mediante la relación\n",
    "is_a.\n",
    "*/\n",
    "pub fn collect_ancestors(\n",
    "    go_id: &str,                            // Identificador de GoTerms.\n",
    "    terms: &HashMap<String, GOTerm>         // Toda la población de términos.\n",
    ") -> HashSet<String> {\n",
    "    // Se intenta revisar si ua se identificaron esos ancestros.\n",
    "    if let Some(lock) = crate::go_loader::ANCESTORS_CACHE.get() {\n",
    "        let cache = lock.read();\n",
    "        if let Some(ancestors) = cache.get(go_id) {\n",
    "            return ancestors.clone();\n",
    "        }\n",
    "    }\n",
    "    // Se obtienen los padres del GOTerm revisando los padres\n",
    "    // recursivamente hasta que solo queden los elementos raíz.\n",
    "    let mut visited = HashSet::default();\n",
    "    let mut stack = vec![go_id];\n",
    "    while let Some(current) = stack.pop() {\n",
    "        if visited.insert(current.to_string()) {\n",
    "            if let Some(term) = terms.get(current) {\n",
    "                for parent in &term.parents {\n",
    "                    stack.push(parent);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    // Ancestros.\n",
    "    visited\n",
    "}\n",
    "\n",
    "/** ancestors (function)\n",
    "Esto cuenta como un encapsulamiento del código con la finalidad de que\n",
    "solo el usuario provea el GO id.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn ancestors(go_id: &str) -> PyResult<Vec<String>> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let visited = collect_ancestors(go_id, &terms);\n",
    "    Ok(visited.into_iter().collect())\n",
    "}\n",
    "\n",
    "/** common_ancestor (function)\n",
    "Esta función lo que realiza es la intersección de todos los términos\n",
    "ancestrales entre dos GO ids.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn common_ancestor(\n",
    "    go_id1: &str,                       // Identificador GOTerm 1\n",
    "    go_id2: &str                        // Identificador GOTerm 2\n",
    ") -> PyResult<Vec<String>> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let set1 = collect_ancestors(go_id1, &terms);\n",
    "    let set2 = collect_ancestors(go_id2, &terms);\n",
    "    let mut common: Vec<String> = set1.intersection(&set2).map(|s| (*s).to_string()).collect();\n",
    "    common.sort_unstable();\n",
    "    Ok(common)\n",
    "}\n",
    "\n",
    "/** deepest_common_ancestor (function)\n",
    "Obtiene el ancestro más profundo entre 2 GOTerms.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn deepest_common_ancestor(go_id1: &str, go_id2: &str) -> PyResult<Option<String>> {\n",
    "    let terms = get_terms_or_error()?;                                      // Se buscan términos dentro de la cache existente.\n",
    "\n",
    "    if !terms.contains_key(go_id1) {                                        // Verificación de que los terminos existan dentro del mapeo de términos.\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id1\n",
    "        )));\n",
    "    }\n",
    "    if !terms.contains_key(go_id2) {\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"GO term '{}' not found in ontology\",\n",
    "            go_id2\n",
    "        )));\n",
    "    }\n",
    "\n",
    "    // Se establece el par ordenado de ambos ids, ya que esto estandariza\n",
    "    // los pares para consulta en cache, como estandarizar que el valor\n",
    "    // es independiente del orden.\n",
    "    let (id_a, id_b) = if go_id1 <= go_id2 {\n",
    "        (go_id1, go_id2)\n",
    "    } else {\n",
    "        (go_id2, go_id1)\n",
    "    };\n",
    "\n",
    "    // Verificar si en caché existió un precalculo del DCA previamente.\n",
    "    if let Some(lock) = crate::go_loader::DCA_CACHE.get() {\n",
    "        let cache = lock.write();\n",
    "        if let Some(result) = cache.get(&(id_a.to_string(), id_b.to_string())) {\n",
    "            return Ok(Some(result.clone()));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // En caso de no encontrar ancestro en caché.\n",
    "    let set1 = collect_ancestors(id_a, &terms);                             // Ancestros del id_a.\n",
    "    let set2 = collect_ancestors(id_b, &terms);                             // Ancestros del id_b.\n",
    "    let mut best = None;\n",
    "    let mut max_depth = 0;\n",
    "    for term_id in set1.intersection(&set2) {                               // Intersección entre ambas listas de ancestros.\n",
    "        if let Some(term) = terms.get(term_id) {                            // Se verifican campos depth para obtener el DCA.\n",
    "            if let Some(depth) = term.depth {\n",
    "                if depth >= max_depth {\n",
    "                    max_depth = depth;\n",
    "                    best = Some(term_id.to_string());\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Guardar reconocimiento de ancestro en caché.\n",
    "    if let Some(lock) = crate::go_loader::DCA_CACHE.get() {\n",
    "        let mut cache = lock.write();\n",
    "        if let Some(ref dca) = best {\n",
    "            cache.insert((id_a.to_string(), id_b.to_string()), dca.clone());\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Ok(best)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f6a07",
   "metadata": {},
   "source": [
    "## Conjunto de códigos - go_semantic\n",
    "En versiones iniciales de la librería el archivo semantic.rs proporcionaba las funciones de cálculo de similitud, no obstante, tras una actualización se proporcionó un conjunto de archivos que realizan esta actividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c6fae",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "// mod.rs: Archivo encargado de proporcionar todas las funcionalidades de la carpeta de forma\n",
    "// que estas solo se consideren como un único conjunto invocable.\n",
    "mod embedding;\n",
    "mod gene;\n",
    "mod similarity;\n",
    "mod termset;\n",
    "\n",
    "// Especificación de funciones asociadas a go_semantic.\n",
    "pub use embedding::{plot_embedding, plot_tsne_genes, plot_umap_genes, tsne_genes, umap_genes};\n",
    "pub use gene::{compare_gene_pairs_batch, compare_genes, gene_distance_matrix};\n",
    "pub use similarity::{batch_similarity, semantic_similarity, set_num_threads, term_ic};\n",
    "pub use termset::termset_similarity;\n",
    "\n",
    "// Se establece función de limieza de caché.\n",
    "pub(crate) fn clear_internal_caches() {\n",
    "    similarity::clear_internal_caches();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e47c0",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "//------------------------------- Liberías.\n",
    "use pyo3::exceptions::PyValueError;                                                     // Invocación de errores ValueError para usuario.\n",
    "use pyo3::prelude::*;                                                                   // Usar Rust en Python.\n",
    "use rayon::prelude::*;                                                                  // Rust comprende Python.\n",
    "use rayon::ThreadPoolBuilder;                                                           // Administración de hilos.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};                           // Implementación de tablas hash.\n",
    "\n",
    "use crate::go_loader::{GO_TERMS_CACHE, TermCounter};                                    // Utilizar estructuras cache de go_loader.\n",
    "// Utilizar elementos asociados a código de go_ontology.\n",
    "use crate::go_ontology::{collect_ancestors, deepest_common_ancestor, get_term_by_id, GOTerm};\n",
    "use dashmap::DashMap;                                                                   // Tablas hash que se les aplica concurrencia.\n",
    "use std::sync::Arc;                                                                     // Sincronización de hilos.\n",
    "\n",
    "\n",
    "//------------------------------- Estructuras.\n",
    "/*Encargada de leer la contribución semántica.*/\n",
    "static SEMANTIC_CONTRIB_CACHE: once_cell::sync::Lazy<\n",
    "    DashMap<String, Arc<HashMap<String, f64>>>,\n",
    "> = once_cell::sync::Lazy::new(DashMap::new);\n",
    "\n",
    "pub(crate) fn clear_internal_caches() {\n",
    "    SEMANTIC_CONTRIB_CACHE.clear();\n",
    "}\n",
    "\n",
    "//------------------------------- Funciones.\n",
    "\n",
    "/** set_num_threads (function)\n",
    "Configuración de hilos que Rust puede utilizar de la máquina.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn set_num_threads(n_threads: usize) -> PyResult<()> {\n",
    "    let builder = if n_threads == 0 {\n",
    "        ThreadPoolBuilder::new()                            // En caso de colocar 0 o nada se utilizan todos.\n",
    "    } else {\n",
    "        ThreadPoolBuilder::new().num_threads(n_threads)     // Si se especifica se establece el número de hilos requeridos.\n",
    "    };\n",
    "\n",
    "    //Se revisa si esta inicialización fue hecha previamente por\n",
    "    //el proceso para evitar una creación indiscriminada de hilos.\n",
    "    match builder.build_global() {\n",
    "        Ok(()) => Ok(()),\n",
    "        Err(e) => {\n",
    "            let msg = e.to_string();\n",
    "            if msg.contains(\"already been initialized\") {\n",
    "                Ok(())\n",
    "            } else {\n",
    "                Err(pyo3::exceptions::PyRuntimeError::new_err(msg))\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "/** term_ic (function)\n",
    "Se obtienen los information content (IC) ya precomputados de \n",
    "go_loader.\n",
    "*/\n",
    "#[pyfunction]\n",
    "#[pyo3(text_signature = \"(go_id, counter)\")]\n",
    "pub fn term_ic(go_id: &str, counter: &TermCounter) -> f64 {\n",
    "    *counter.ic.get(go_id).unwrap_or(&0.0)\n",
    "}\n",
    "\n",
    "/*\n",
    "Esta sección de código administra la invocación de algún método\n",
    "de similitud según el texto que ingrese el usuario en los parámetros\n",
    "al utilizar la función en Python.\n",
    "*/\n",
    "#[derive(Debug, Clone, Copy)]\n",
    "pub(crate) enum SimilarityMethod {\n",
    "    Resnik,\n",
    "    Lin,\n",
    "    JC,\n",
    "    SimRel,\n",
    "    ICCoef,\n",
    "    GraphIC,\n",
    "    Wang,\n",
    "    TopoICSim,\n",
    "}\n",
    "\n",
    "/** SimilarityMethod (Interfaz de funciones)\n",
    "En este código se invocan todos los cálculos de métodos de similitud solicitados por el\n",
    "usuario, en resumidos término este código obedece todas las fórmulas planteadas en la \n",
    "documentación provista por el autor, por ende, se recomienda guiarse por esa información.\n",
    "*/\n",
    "impl SimilarityMethod {\n",
    "    pub(crate) fn from_str(name: &str) -> Option<Self> {\n",
    "        match name.to_ascii_lowercase().as_str() {\n",
    "            \"resnik\" => Some(SimilarityMethod::Resnik),\n",
    "            \"lin\" => Some(SimilarityMethod::Lin),\n",
    "            \"jc\" => Some(SimilarityMethod::JC),\n",
    "            \"simrel\" => Some(SimilarityMethod::SimRel),\n",
    "            \"iccoef\" => Some(SimilarityMethod::ICCoef),\n",
    "            \"graphic\" => Some(SimilarityMethod::GraphIC),\n",
    "            \"wang\" => Some(SimilarityMethod::Wang),\n",
    "            \"topoicsim\" => Some(SimilarityMethod::TopoICSim),\n",
    "            _ => None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pub(crate) fn compute(&self, id1: &str, id2: &str, counter: &TermCounter) -> f64 {\n",
    "        match self {\n",
    "            SimilarityMethod::Resnik => {\n",
    "                let dca = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => dca,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                *counter.ic.get(&dca).unwrap_or(&0.0)\n",
    "            }\n",
    "            SimilarityMethod::Lin => {\n",
    "                let dca = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => dca,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                if id1 == id2 {\n",
    "                    return 1.0\n",
    "                }\n",
    "                let resnik = *counter.ic.get(&dca).unwrap_or(&0.0);\n",
    "                if resnik == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                let ic1 = *counter.ic.get(id1).unwrap_or(&0.0);\n",
    "                let ic2 = *counter.ic.get(id2).unwrap_or(&0.0);\n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                2.0 * resnik / (ic1 + ic2)\n",
    "            }\n",
    "            SimilarityMethod::JC => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                let distance = ic1 + ic2 - 2.0 * dca_ic;\n",
    "                if distance <= 0.0 {\n",
    "                    return f64::INFINITY;  // Máxima similitud\n",
    "                }\n",
    "                if distance.is_infinite() {\n",
    "                    0.0\n",
    "                } else {\n",
    "                    1.0 / (1.0 + distance)\n",
    "                }\n",
    "            }\n",
    "            SimilarityMethod::SimRel => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if dca_ic == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let lin = (2.0 * dca_ic) / (ic1 + ic2);\n",
    "                lin * (1.0 - (-dca_ic).exp())\n",
    "            }\n",
    "            SimilarityMethod::ICCoef => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let ic1 = term_ic(id1, counter);\n",
    "                let ic2 = term_ic(id2, counter);\n",
    "            \n",
    "                if ic1 == 0.0 || ic2 == 0.0 {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                dca_ic / ic1.min(ic2)\n",
    "            }\n",
    "            SimilarityMethod::GraphIC => {\n",
    "                let (t1, t2) = match (get_term_by_id(id1).ok(), get_term_by_id(id2).ok()) {\n",
    "                    (Some(t1), Some(t2)) => (t1, t2),\n",
    "                    _ => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let depth1 = t1.depth.unwrap_or(0);\n",
    "                let depth2 = t2.depth.unwrap_or(0);\n",
    "                let max_depth = (depth1.max(depth2) + 1) as f64;\n",
    "            \n",
    "                let dca_ic = match deepest_common_ancestor(id1, id2).ok().flatten() {\n",
    "                    Some(dca) => term_ic(&dca, counter),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                dca_ic / max_depth\n",
    "            }\n",
    "            SimilarityMethod::Wang => {\n",
    "                let terms = match GO_TERMS_CACHE.get() {\n",
    "                    Some(lock) => lock.read(),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                let terms = &*terms;\n",
    "            \n",
    "                let t1 = match terms.get(id1) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t2 = match terms.get(id2) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "            \n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "            \n",
    "                let sv_a = semantic_contributions(id1, terms);\n",
    "                let sv_b = semantic_contributions(id2, terms);\n",
    "\n",
    "                let sum_a: f64 = sv_a.values().sum();\n",
    "                let sum_b: f64 = sv_b.values().sum();\n",
    "\n",
    "                let (small, large) = if sv_a.len() <= sv_b.len() {\n",
    "                    (&*sv_a, &*sv_b)\n",
    "                } else {\n",
    "                    (&*sv_b, &*sv_a)\n",
    "                };\n",
    "\n",
    "                let mut numerator = 0.0;\n",
    "                for (key, w1) in small.iter() {\n",
    "                    if let Some(w2) = large.get(key) {\n",
    "                        numerator += (*w1).min(*w2);\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "                if sum_a + sum_b == 0.0 {\n",
    "                    0.0\n",
    "                } else {\n",
    "                    numerator / ((sum_a + sum_b) / 2.0)\n",
    "                }\n",
    "            }\n",
    "            SimilarityMethod::TopoICSim => {\n",
    "                // Get terms and check namespace\n",
    "                let terms = match GO_TERMS_CACHE.get() {\n",
    "                    Some(lock) => lock.read(),\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t1 = match terms.get(id1) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                let t2 = match terms.get(id2) {\n",
    "                    Some(t) => t,\n",
    "                    None => return 0.0,\n",
    "                };\n",
    "                if t1.namespace != t2.namespace {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Disjunctive common ancestors\n",
    "                let dca_set = disjunctive_common_ancestors(id1, id2, &terms);\n",
    "                if dca_set.is_empty() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Find all roots\n",
    "                let roots = find_roots(&terms);\n",
    "                if roots.is_empty() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                let mut min_d = f64::INFINITY;\n",
    "                for x in dca_set {\n",
    "                    // Weighted shortest path from t1 to x and t2 to x\n",
    "                    let wsp1 = weighted_shortest_path_iic(id1, &x, &terms, counter);\n",
    "                    let wsp2 = weighted_shortest_path_iic(id2, &x, &terms, counter);\n",
    "                    if wsp1.is_none() || wsp2.is_none() {\n",
    "                        continue;\n",
    "                    }\n",
    "                    let wsp = wsp1.unwrap() + wsp2.unwrap();\n",
    "                    // Weighted longest path from x to any root (take the max over all roots)\n",
    "                    let mut max_wlp = None;\n",
    "                    for root in &roots {\n",
    "                        if let Some(wlp) = weighted_longest_path_iic(&x, root, &terms, counter) {\n",
    "                            max_wlp = Some(max_wlp.map_or(wlp, |m: f64| m.max(wlp)));\n",
    "                        }\n",
    "                    }\n",
    "                    let wlp = match max_wlp {\n",
    "                        Some(val) if val > 0.0 => val,\n",
    "                        _ => continue,\n",
    "                    };\n",
    "                    let d = wsp / wlp;\n",
    "                    if d < min_d {\n",
    "                        min_d = d;\n",
    "                    }\n",
    "                }\n",
    "                if !min_d.is_finite() {\n",
    "                    return 0.0;\n",
    "                }\n",
    "                // Similarity formula: 1 - (arctan(D) / (pi/2))\n",
    "                let sim = 1.0 - (min_d.atan() / (std::f64::consts::FRAC_PI_2));\n",
    "                if sim.is_finite() && sim > 0.0 { sim } else { 0.0 }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/** semantic_contributions (function)\n",
    "Realiza el cálculo de la contribución de todos los ancestros asociados\n",
    "a un GOTerm según los valores establecidos en literatura. Este componente\n",
    "solo se utiliza para el índice de Wang.\n",
    "*/\n",
    "fn semantic_contributions(\n",
    "    go_id: &str,                                                // Identificador de GoTerm.\n",
    "    terms: &HashMap<String, GOTerm>,                            // Conjunto de términos GO.\n",
    ") -> Arc<HashMap<String, f64>> {\n",
    "    // Verifica si el cálculo ya esta en caché.\n",
    "    if let Some(cached) = SEMANTIC_CONTRIB_CACHE.get(go_id) {\n",
    "        return Arc::clone(cached.value());\n",
    "    }\n",
    "\n",
    "    // Inicialización de variables.\n",
    "    let mut contributions = HashMap::default();                 // Contribuyentes.\n",
    "    let mut to_visit: Vec<(&str, f64)> = vec![(go_id, 1.0)];    // Primer término de la lista es el mismo de entrada.\n",
    "\n",
    "    while let Some((current_id, weight)) = to_visit.pop() {\n",
    "        // Si la contribución es muy pequeña, se omite.\n",
    "        if weight < 1e-6 || contributions.contains_key(current_id) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        contributions.insert(current_id.to_string(), weight);\n",
    "\n",
    "        if let Some(term) = terms.get(current_id) {\n",
    "            // is_a → 0.8 si es padre.\n",
    "            for parent in &term.parents {\n",
    "                to_visit.push((parent.as_str(), weight * 0.8));\n",
    "            }\n",
    "            // part_of → 0.6 si es relacionado.\n",
    "            for (rel_type, target) in &term.relationships {\n",
    "                if rel_type == \"part_of\" {\n",
    "                    to_visit.push((target.as_str(), weight * 0.6));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Contribuciones se guarda en caché.\n",
    "    let contributions = Arc::new(contributions);\n",
    "    SEMANTIC_CONTRIB_CACHE.insert(go_id.to_string(), Arc::clone(&contributions));\n",
    "    contributions\n",
    "}\n",
    "\n",
    "\n",
    "//------------------------------- Funciones TopoICSim.\n",
    "\n",
    "/** iic (function)\n",
    "Calcula el Inverse Information Content (IIC) para un término.\n",
    "*/\n",
    "fn iic(go_id: &str, counter: &TermCounter) -> f64 {\n",
    "    let ic = *counter.ic.get(go_id).unwrap_or(&0.0);\n",
    "    if ic > 0.0 {\n",
    "        1.0 / ic\n",
    "    } else {\n",
    "        // Si el IC es 0, el IIC se trata como un valor extremadamente alto.\n",
    "        1e12\n",
    "    }\n",
    "}\n",
    "\n",
    "/** weighted_shortest_path_iic (function)\n",
    "Se calcula el camino más corto según IIC entre dos GOTerms.\n",
    "*/\n",
    "fn weighted_shortest_path_iic(\n",
    "    source: &str,                                   // GoTerm de origen.\n",
    "    target: &str,                                   // Goterm objetivo.\n",
    "    terms: &HashMap<String, GOTerm>,                // Conjunto de términos.\n",
    "    counter: &TermCounter,                          // TermCounter.\n",
    ") -> Option<f64> {\n",
    "    // Usamos estructuras estándar: un heap (cola de prioridad) y un HashMap local.\n",
    "    use std::collections::{BinaryHeap, HashMap};\n",
    "    use std::cmp::Ordering;\n",
    "\n",
    "    // Estado que guardamos en el heap:\n",
    "    // - `cost`: costo acumulado para llegar a este nodo\n",
    "    // - `node`: índice numérico del nodo (más eficiente que usar String)\n",
    "    #[derive(Copy, Clone, PartialEq)]\n",
    "    struct State {\n",
    "        cost: f64,\n",
    "        node: usize,\n",
    "    }\n",
    "\n",
    "    // Necesario para que `State` pueda ir dentro de BinaryHeap.\n",
    "    impl Eq for State {}\n",
    "\n",
    "    // `BinaryHeap` en Rust es un *max-heap* por defecto (saca el mayor primero).\n",
    "    // Para que se comporte como *min-heap* (sacar el menor costo primero),\n",
    "    // invertimos la comparación (`other` vs `self`).\n",
    "    impl Ord for State {\n",
    "        fn cmp(&self, other: &Self) -> Ordering {\n",
    "            // Orden invertido → el menor `cost` queda “más alto” en prioridad.\n",
    "            other.cost.partial_cmp(&self.cost).unwrap_or(Ordering::Equal)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    impl PartialOrd for State {\n",
    "        fn partial_cmp(&self, other: &Self) -> Option<Ordering> {\n",
    "            Some(self.cmp(other))\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // 1) Construir un mapeo entre IDs (String) y índices (usize)\n",
    "    //\n",
    "    // ¿Por qué?\n",
    "    // - `dist` será un Vec<f64> (mucho más rápido que HashMap en cada actualización)\n",
    "    // - Para usar Vec necesitamos índices numéricos.\n",
    "    // -------------------------------------------------------------------------\n",
    "    let mut id2idx = HashMap::new();            // GO_ID -> índice\n",
    "    let mut idx2id = Vec::new();                // índice -> GO_ID\n",
    "\n",
    "    for (i, id) in terms.keys().enumerate() {\n",
    "        id2idx.insert(id.as_str(), i);\n",
    "        idx2id.push(id.as_str());\n",
    "    }\n",
    "\n",
    "    // Verificar que `source` y `target` existen en el grafo.\n",
    "    let src = match id2idx.get(source) {\n",
    "        Some(&i) => i,\n",
    "        None => return None,\n",
    "    };\n",
    "    let tgt = match id2idx.get(target) {\n",
    "        Some(&i) => i,\n",
    "        None => return None,\n",
    "    };\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // 2) Inicializar estructuras de Dijkstra\n",
    "    //\n",
    "    // `dist[i]` = mejor (menor) costo conocido para llegar al nodo i.\n",
    "    // Partimos con infinito para todos, excepto `source`.\n",
    "    // -------------------------------------------------------------------------\n",
    "    let mut dist = vec![f64::INFINITY; idx2id.len()];\n",
    "\n",
    "    // Costo inicial:\n",
    "    // - En esta implementación, el costo incluye el IIC del nodo inicial.\n",
    "    dist[src] = iic(source, counter);\n",
    "\n",
    "    // Heap (cola de prioridad) con el nodo inicial.\n",
    "    let mut heap = BinaryHeap::new();\n",
    "    heap.push(State { cost: dist[src], node: src });\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // 3) Bucle principal de Dijkstra\n",
    "    //\n",
    "    // Siempre tomamos el nodo pendiente con menor costo acumulado.\n",
    "    // -------------------------------------------------------------------------\n",
    "    while let Some(State { cost, node }) = heap.pop() {\n",
    "        let node_id = idx2id[node];\n",
    "\n",
    "        // Si ya llegamos al destino, el costo actual es el menor posible.\n",
    "        if node == tgt {\n",
    "            return Some(cost);\n",
    "        }\n",
    "\n",
    "        // Si este estado no coincide con la mejor distancia conocida, lo ignoramos.\n",
    "        // (Esto ocurre porque en el heap pueden quedar entradas antiguas)\n",
    "        if cost > dist[node] {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // ---------------------------------------------------------------------\n",
    "        // 4) Explorar vecinos (en GO: subir hacia los padres)\n",
    "        //\n",
    "        // Aquí el “vecino” de un nodo son sus padres.\n",
    "        // Cada paso añade el peso IIC del padre al costo acumulado.\n",
    "        // ---------------------------------------------------------------------\n",
    "        if let Some(term) = terms.get(node_id) {\n",
    "            for parent in &term.parents {\n",
    "                if let Some(&parent_idx) = id2idx.get(parent.as_str()) {\n",
    "                    // Nuevo costo candidato si vamos desde `node` hacia `parent`.\n",
    "                    let next = cost + iic(parent, counter);\n",
    "\n",
    "                    // Si encontramos un camino más barato hacia el padre, lo guardamos.\n",
    "                    if next < dist[parent_idx] {\n",
    "                        dist[parent_idx] = next;\n",
    "                        heap.push(State { cost: next, node: parent_idx });\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Si se vació el heap y nunca llegamos a `target`, no hay conexión.\n",
    "    None\n",
    "\n",
    "}\n",
    "\n",
    "/// Compute the weighted longest path (sum of IICs) from source to target (ancestor) in the GO DAG.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// source : str\n",
    "///   Source GO term ID.\n",
    "/// target : str\n",
    "///   Target GO term ID (ancestor).\n",
    "/// terms : dict\n",
    "///   Map of GO terms.\n",
    "/// counter : TermCounter\n",
    "///   Precomputed term counter with IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// Option<float>\n",
    "///   Maximum sum of IICs along any path from source to target, or None if not connected.\n",
    "\n",
    "/** weighted_longest_path_iic (function)\n",
    "Se calcula el camino más largo según IIC entre dos GOTerms.\n",
    "*/\n",
    "fn weighted_longest_path_iic(\n",
    "    source: &str,                                   // GoTerm de origen.\n",
    "    target: &str,                                   // Goterm objetivo.\n",
    "    terms: &HashMap<String, GOTerm>,                // Conjunto de términos.\n",
    "    counter: &TermCounter,                          // TermCounter.\n",
    ") -> Option<f64> {\n",
    "    // Usamos HashMap estándar solo para la memoización local.\n",
    "    use std::collections::HashMap as StdHashMap;\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // DFS con memoización:\n",
    "    // dfs(node) retorna la *máxima* suma de IIC desde `node` hasta `target`.\n",
    "    // -------------------------------------------------------------------------\n",
    "    fn dfs(\n",
    "        node: &str,\n",
    "        target: &str,\n",
    "        terms: &HashMap<String, GOTerm>,\n",
    "        counter: &TermCounter,\n",
    "        memo: &mut StdHashMap<String, Option<f64>>,\n",
    "    ) -> Option<f64> {\n",
    "        // Caso base:\n",
    "        // - Si ya estamos en el destino, el mejor camino es “quedarse aquí”.\n",
    "        // - Incluimos el IIC del propio `target`.\n",
    "        if node == target {\n",
    "            return Some(iic(node, counter));\n",
    "        }\n",
    "\n",
    "        // Si ya calculamos la respuesta para `node`, la reutilizamos.\n",
    "        // Esto evita recomputar el mismo subgrafo muchas veces.\n",
    "        if let Some(&val) = memo.get(node) {\n",
    "            return val;\n",
    "        }\n",
    "\n",
    "        // Aquí iremos guardando el mejor (máximo) camino encontrado.\n",
    "        let mut max_path = None;\n",
    "\n",
    "        // Si el nodo existe, probamos cada padre como “siguiente paso”.\n",
    "        if let Some(term) = terms.get(node) {\n",
    "            // Preguntamos: “¿Existe un camino desde el padre hasta target?”\n",
    "            for parent in &term.parents {\n",
    "                if let Some(sub) = dfs(parent, target, terms, counter, memo) {\n",
    "                    // Si existe, el camino total desde `node` es:\n",
    "                    // IIC(node) + (mejor camino desde parent hasta target)\n",
    "                    let total = iic(node, counter) + sub;\n",
    "\n",
    "                    // Nos quedamos con el máximo entre los caminos posibles.\n",
    "                    max_path = Some(max_path.map_or(total, |m: f64| m.max(total)));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        // Guardamos el resultado para `node` en memo (aunque sea None).\n",
    "        memo.insert(node.to_string(), max_path);\n",
    "        max_path\n",
    "    }\n",
    "\n",
    "    // Memo vacía al inicio.\n",
    "    let mut memo = StdHashMap::new();\n",
    "\n",
    "    // Ejecutamos la DFS desde `source`.\n",
    "    dfs(source, target, terms, counter, &mut memo)\n",
    "}\n",
    "\n",
    "/** disjunctive_common_ancestors (function)\n",
    "Obtener los ancestros comunes disyuntivos entre un par de GOTerms.\n",
    "*/\n",
    "fn disjunctive_common_ancestors(\n",
    "    id1: &str,\n",
    "    id2: &str,\n",
    "    terms: &HashMap<String, GOTerm>,\n",
    ") -> Vec<String> {\n",
    "    use std::collections::HashSet;\n",
    "    // Se obtiene todos los ancestros comunes entre los dos términos de entrada.\n",
    "    let ancestors1 = collect_ancestors(id1, terms);\n",
    "    let ancestors2 = collect_ancestors(id2, terms);\n",
    "    let common: HashSet<_> = ancestors1.intersection(&ancestors2).cloned().collect();\n",
    "\n",
    "    // Aquí se comprueba la consición de disyuntivo, es decir, que el término es\n",
    "    // un ancestro de ambos GOTerms pero sus hijos no son ancestros compartidos.\n",
    "    let mut dca = Vec::new();\n",
    "    for x in &common {\n",
    "        let is_disjunctive = terms.get(x).map_or(false, |term| {\n",
    "            term.children.iter().all(|c| !common.contains(c))\n",
    "        });\n",
    "        if is_disjunctive {\n",
    "            dca.push(x.clone());\n",
    "        }\n",
    "    }\n",
    "    dca\n",
    "}\n",
    "\n",
    "\n",
    "/** find_roots (function)\n",
    "Obtener los términos raíz (sin padres) del conjunto total de GoTerms.\n",
    "*/\n",
    "fn find_roots(terms: &HashMap<String, GOTerm>) -> Vec<String> {\n",
    "    terms.iter()\n",
    "        .filter(|(_, term)| term.parents.is_empty())\n",
    "        .map(|(id, _)| id.clone())\n",
    "        .collect()\n",
    "}\n",
    "\n",
    "\n",
    "/** semantic_similarity (function)\n",
    "Función capsula para el cálculo de similitud entre 2 GOTerms.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn semantic_similarity(\n",
    "    id1: &str,\n",
    "    id2: &str,\n",
    "    method: &str,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<f64> {\n",
    "\n",
    "    let method_enum = SimilarityMethod::from_str(method)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", method)))?;\n",
    "\n",
    "    Ok(method_enum.compute(id1, id2, counter))\n",
    "}\n",
    "\n",
    "/** semantic_similarity (function)\n",
    "Función capsula para el cálculo de similitud entre 2 listas de GOTerms.\n",
    "*/\n",
    "#[pyfunction]\n",
    "pub fn batch_similarity(\n",
    "    list1: Vec<String>,\n",
    "    list2: Vec<String>,\n",
    "    method: &str,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<Vec<f64>> {\n",
    "    // Se comprueba que ambas listas tengan el mismo tamaño.\n",
    "    if list1.len() != list2.len() {\n",
    "        return Err(PyValueError::new_err(\"Both lists must be the same length\"));\n",
    "    }\n",
    "\n",
    "    // Se establece método de similitud según lo indicado en los parametros\n",
    "    // si no se obtiene respuesta, entonces se señala que el método escrito no es\n",
    "    // valido.\n",
    "    let method_enum = SimilarityMethod::from_str(method)\n",
    "        .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", method)))?;\n",
    "\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // \"Interning\" de términos: asignar a cada GO ID un índice numérico.\n",
    "    //\n",
    "    // ¿Por qué?\n",
    "    // - Para evitar copiar Strings y usarlas como keys una y otra vez.\n",
    "    // - Trabajar con `usize` (índices) es más rápido y barato en memoria.\n",
    "    //\n",
    "    // term_to_idx: &str (GO ID) -> índice\n",
    "    // idx_to_term: índice -> &str (GO ID)\n",
    "    //\n",
    "    // Nota importante:\n",
    "    // - Estos &str apuntan a las Strings dentro de list1/list2.\n",
    "    // - Esto es válido porque list1/list2 viven durante toda la función.\n",
    "    // ---------------------------------------------------------------------\n",
    "    let mut term_to_idx: HashMap<&str, usize> = HashMap::default();\n",
    "    let mut idx_to_term: Vec<&str> = Vec::new();\n",
    "    for s in list1.iter().chain(list2.iter()) {\n",
    "        let key = s.as_str();\n",
    "        if term_to_idx.get(key).is_none() {\n",
    "            let idx = idx_to_term.len();\n",
    "            term_to_idx.insert(key, idx);\n",
    "            idx_to_term.push(key);\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // ---------------------------------------------------------------------\n",
    "    // Recolectar pares únicos (sin orden) para no recalcular similitudes.\n",
    "    //\n",
    "    // Si aparecen pares repetidos, por ejemplo:\n",
    "    //   (A, B) muchas veces\n",
    "    // solo calculamos una vez su similitud.\n",
    "    //\n",
    "    // \"sin orden\" significa:\n",
    "    // - (A,B) y (B,A) se consideran el mismo par\n",
    "    // - se normaliza usando (min(i,j), max(i,j))\n",
    "    //\n",
    "    // unique_pairs: conjunto de pares únicos ya normalizados (i <= j)\n",
    "    // pair_indices: lista de pares normalizados en el mismo orden del input,\n",
    "    //               para luego reconstruir el resultado por posición.\n",
    "    // ---------------------------------------------------------------------\n",
    "    let mut unique_pairs: HashSet<(usize, usize)> = HashSet::default();\n",
    "    let mut pair_indices: Vec<(usize, usize)> = Vec::with_capacity(list1.len());\n",
    "    for (a, b) in list1.iter().zip(list2.iter()) {\n",
    "        let i = *term_to_idx\n",
    "            .get(a.as_str())\n",
    "            .expect(\"intern table should contain all terms in list1\");\n",
    "        let j = *term_to_idx\n",
    "            .get(b.as_str())\n",
    "            .expect(\"intern table should contain all terms in list2\");\n",
    "        let key = if i <= j { (i, j) } else { (j, i) };\n",
    "        unique_pairs.insert(key);\n",
    "        pair_indices.push(key);\n",
    "    }\n",
    "\n",
    "    // Computación de las similitudes entre pares de forma paralela.\n",
    "    let sim_map: HashMap<(usize, usize), f64> = unique_pairs\n",
    "        .par_iter()\n",
    "        .map(|(i, j)| {\n",
    "            let a = idx_to_term[*i];\n",
    "            let b = idx_to_term[*j];\n",
    "            let sim = method_enum.compute(a, b, counter);\n",
    "            ((*i, *j), sim)\n",
    "        })\n",
    "        .collect();\n",
    "    let result: Vec<f64> = pair_indices\n",
    "        .par_iter()\n",
    "        .map(|(i, j)| *sim_map.get(&(*i, *j)).unwrap_or(&0.0))\n",
    "        .collect();\n",
    "\n",
    "    Ok(result)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df986717",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "//------------------------------- Liberías.\n",
    "use pyo3::exceptions::PyValueError;                                                     // Invocación de errores ValueError para usuario.\n",
    "use pyo3::prelude::*;                                                                   // Usar Rust en Python.\n",
    "use rayon::prelude::*;                                                                  // Rust comprende Python.\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};                           // Implementación de tablas hash.\n",
    "\n",
    "// Se obtienen componentes externas de otros archivos\n",
    "use crate::go_loader::TermCounter;                                                      // Term Counter.\n",
    "use crate::go_ontology::{collect_ancestors, get_terms_or_error, GOTerm};                // Manejo del mapeo de GO.\n",
    "use super::similarity::{term_ic, SimilarityMethod};                                     // Uso de IC y métodos de similitud.\n",
    "\n",
    "/** termset_similarity_internal_with_method (function)\n",
    "Esta función esta encargada de la inicialización \n",
    "*/\n",
    "pub(crate) fn termset_similarity_internal_with_method(\n",
    "    terms1: &[String],\n",
    "    terms2: &[String],\n",
    "    sim_fn: Option<SimilarityMethod>,\n",
    "    groupwise: &str,\n",
    "    counter: &TermCounter,\n",
    "    ontology_terms: &HashMap<String, GOTerm>,\n",
    ") -> PyResult<f64> {\n",
    "    if terms1.is_empty() || terms2.is_empty() {\n",
    "        return Ok(0.0);\n",
    "    }\n",
    "\n",
    "    if groupwise == \"simgic\" {\n",
    "        // Collect all ancestors for each set\n",
    "        let mut ancestors1: HashSet<String> = HashSet::default();\n",
    "        for t in terms1 {\n",
    "            let ancs = collect_ancestors(t, ontology_terms);\n",
    "            for a in ancs {\n",
    "                ancestors1.insert(a);\n",
    "            }\n",
    "        }\n",
    "        let mut ancestors2: HashSet<String> = HashSet::default();\n",
    "        for t in terms2 {\n",
    "            let ancs = collect_ancestors(t, ontology_terms);\n",
    "            for a in ancs {\n",
    "                ancestors2.insert(a);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Compute Jaccard Index weighted by IC\n",
    "        let mut intersection_ic = 0.0;\n",
    "        let mut union_ic = 0.0;\n",
    "        \n",
    "        let all_ancestors: HashSet<&String> = ancestors1.union(&ancestors2).collect();\n",
    "\n",
    "        for term in all_ancestors {\n",
    "            let ic = term_ic(term, counter);\n",
    "            let in_1 = ancestors1.contains(term);\n",
    "            let in_2 = ancestors2.contains(term);\n",
    "            \n",
    "            if in_1 && in_2 {\n",
    "                intersection_ic += ic;\n",
    "            }\n",
    "            if in_1 || in_2 {\n",
    "                union_ic += ic;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if union_ic == 0.0 {\n",
    "            return Ok(0.0);\n",
    "        }\n",
    "        return Ok(intersection_ic / union_ic);\n",
    "    }\n",
    "    \n",
    "    // For other methods, we need the pairwise similarity function\n",
    "    let sim_fn = sim_fn.ok_or_else(|| {\n",
    "        PyValueError::new_err(\"similarity argument is required for this groupwise method\")\n",
    "    })?;\n",
    "\n",
    "    // Avoid nested rayon parallelism when this function is called from an already-parallel\n",
    "    // context (e.g. gene_distance_matrix or compare_gene_pairs_batch). In those cases, the\n",
    "    // outer loop should control parallelism, and we compute the termset score serially.\n",
    "    let use_parallel = rayon::current_thread_index().is_none();\n",
    "\n",
    "    match groupwise {\n",
    "        \"max\" => {\n",
    "            let max_val = if use_parallel {\n",
    "                terms1\n",
    "                    .par_iter()\n",
    "                    .map(|id1| {\n",
    "                        terms2\n",
    "                            .iter()\n",
    "                            .map(|id2| sim_fn.compute(id1, id2, counter))\n",
    "                            .fold(0.0, f64::max)\n",
    "                    })\n",
    "                    .reduce(|| 0.0, f64::max)\n",
    "            } else {\n",
    "                let mut max_val: f64 = 0.0;\n",
    "                for id1 in terms1 {\n",
    "                    for id2 in terms2 {\n",
    "                        max_val = max_val.max(sim_fn.compute(id1, id2, counter));\n",
    "                    }\n",
    "                }\n",
    "                max_val\n",
    "            };\n",
    "            Ok(max_val)\n",
    "        }\n",
    "        \"bma\" => {\n",
    "            let total = (terms1.len() + terms2.len()) as f64;\n",
    "            if total == 0.0 {\n",
    "                return Ok(0.0);\n",
    "            }\n",
    "\n",
    "            // Compute row maxima for terms1 and column maxima for terms2 in a single pass\n",
    "            // over the cartesian product. This avoids doing 2× work for symmetric similarities.\n",
    "            if use_parallel {\n",
    "                let (sum_row_max, col_max) = terms1\n",
    "                    .par_iter()\n",
    "                    .fold(\n",
    "                        || (0.0_f64, vec![0.0_f64; terms2.len()]),\n",
    "                        |(sum, mut col_max), id1| {\n",
    "                            let mut row_max: f64 = 0.0;\n",
    "                            for (j, id2) in terms2.iter().enumerate() {\n",
    "                                let s = sim_fn.compute(id1, id2, counter);\n",
    "                                if s > row_max {\n",
    "                                    row_max = s;\n",
    "                                }\n",
    "                                if s > col_max[j] {\n",
    "                                    col_max[j] = s;\n",
    "                                }\n",
    "                            }\n",
    "                            (sum + row_max, col_max)\n",
    "                        },\n",
    "                    )\n",
    "                    .reduce(\n",
    "                        || (0.0_f64, vec![0.0_f64; terms2.len()]),\n",
    "                        |(sum_a, mut col_a), (sum_b, col_b)| {\n",
    "                            for (a, b) in col_a.iter_mut().zip(col_b.iter()) {\n",
    "                                if *b > *a {\n",
    "                                    *a = *b;\n",
    "                                }\n",
    "                            }\n",
    "                            (sum_a + sum_b, col_a)\n",
    "                        },\n",
    "                    );\n",
    "                let sum_col_max: f64 = col_max.into_iter().sum();\n",
    "                Ok((sum_row_max + sum_col_max) / total)\n",
    "            } else {\n",
    "                let mut col_max = vec![0.0_f64; terms2.len()];\n",
    "                let mut sum_row_max: f64 = 0.0;\n",
    "\n",
    "                for id1 in terms1 {\n",
    "                    let mut row_max: f64 = 0.0;\n",
    "                    for (j, id2) in terms2.iter().enumerate() {\n",
    "                        let s = sim_fn.compute(id1, id2, counter);\n",
    "                        if s > row_max {\n",
    "                            row_max = s;\n",
    "                        }\n",
    "                        if s > col_max[j] {\n",
    "                            col_max[j] = s;\n",
    "                        }\n",
    "                    }\n",
    "                    sum_row_max += row_max;\n",
    "                }\n",
    "\n",
    "                let sum_col_max: f64 = col_max.into_iter().sum();\n",
    "                Ok((sum_row_max + sum_col_max) / total)\n",
    "            }\n",
    "        }\n",
    "        \"avg\" => {\n",
    "             let count = (terms1.len() * terms2.len()) as f64;\n",
    "             if count == 0.0 {\n",
    "                 return Ok(0.0);\n",
    "             }\n",
    "             // sum( sim(t1, t2) ) / (N*M)\n",
    "             let total_sim: f64 = if use_parallel {\n",
    "                 terms1\n",
    "                     .par_iter()\n",
    "                     .map(|id1| {\n",
    "                         terms2\n",
    "                             .iter()\n",
    "                             .map(|id2| sim_fn.compute(id1, id2, counter))\n",
    "                             .sum::<f64>()\n",
    "                     })\n",
    "                     .sum()\n",
    "             } else {\n",
    "                 let mut total = 0.0;\n",
    "                 for id1 in terms1 {\n",
    "                     for id2 in terms2 {\n",
    "                         total += sim_fn.compute(id1, id2, counter);\n",
    "                     }\n",
    "                 }\n",
    "                 total\n",
    "             };\n",
    "                 \n",
    "             Ok(total_sim / count)\n",
    "        }\n",
    "        \"hausdorff\" => {\n",
    "            // min( min_a max_b sim(a, b), min_b max_a sim(b, a) )\n",
    "\n",
    "            let (min_row_max, col_max) = if use_parallel {\n",
    "                terms1\n",
    "                    .par_iter()\n",
    "                    .fold(\n",
    "                        || (f64::INFINITY, vec![0.0_f64; terms2.len()]),\n",
    "                        |(min_row, mut col_max), id1| {\n",
    "                            let mut row_max: f64 = 0.0;\n",
    "                            for (j, id2) in terms2.iter().enumerate() {\n",
    "                                let s = sim_fn.compute(id1, id2, counter);\n",
    "                                if s > row_max {\n",
    "                                    row_max = s;\n",
    "                                }\n",
    "                                if s > col_max[j] {\n",
    "                                    col_max[j] = s;\n",
    "                                }\n",
    "                            }\n",
    "                            (min_row.min(row_max), col_max)\n",
    "                        },\n",
    "                    )\n",
    "                    .reduce(\n",
    "                        || (f64::INFINITY, vec![0.0_f64; terms2.len()]),\n",
    "                        |(min_a, mut col_a), (min_b, col_b)| {\n",
    "                            for (a, b) in col_a.iter_mut().zip(col_b.iter()) {\n",
    "                                if *b > *a {\n",
    "                                    *a = *b;\n",
    "                                }\n",
    "                            }\n",
    "                            (min_a.min(min_b), col_a)\n",
    "                        },\n",
    "                    )\n",
    "            } else {\n",
    "                let mut col_max = vec![0.0_f64; terms2.len()];\n",
    "                let mut min_row_max = f64::INFINITY;\n",
    "\n",
    "                for id1 in terms1 {\n",
    "                    let mut row_max: f64 = 0.0;\n",
    "                    for (j, id2) in terms2.iter().enumerate() {\n",
    "                        let s = sim_fn.compute(id1, id2, counter);\n",
    "                        if s > row_max {\n",
    "                            row_max = s;\n",
    "                        }\n",
    "                        if s > col_max[j] {\n",
    "                            col_max[j] = s;\n",
    "                        }\n",
    "                    }\n",
    "                    min_row_max = min_row_max.min(row_max);\n",
    "                }\n",
    "\n",
    "                (min_row_max, col_max)\n",
    "            };\n",
    "\n",
    "            let min_col_max: f64 = col_max.into_iter().fold(f64::INFINITY, f64::min);\n",
    "\n",
    "            if min_row_max.is_infinite() || min_col_max.is_infinite() {\n",
    "                Ok(0.0)\n",
    "            } else {\n",
    "                Ok(min_row_max.min(min_col_max))\n",
    "            }\n",
    "        }\n",
    "        _ => Err(pyo3::exceptions::PyValueError::new_err(format!(\"Unknown groupwise strategy: {}\", groupwise))),\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Internal helper to compute similarity between two sets of GO terms.\n",
    "pub(crate) fn termset_similarity_internal(\n",
    "    terms1: &[String],\n",
    "    terms2: &[String],\n",
    "    similarity: &str,\n",
    "    groupwise: &str,\n",
    "    counter: &TermCounter,\n",
    "    ontology_terms: &HashMap<String, GOTerm>,\n",
    ") -> PyResult<f64> {\n",
    "    let sim_fn = if groupwise == \"simgic\" {\n",
    "        None\n",
    "    } else {\n",
    "        Some(\n",
    "            SimilarityMethod::from_str(similarity)\n",
    "                .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", similarity)))?\n",
    "        )\n",
    "    };\n",
    "    termset_similarity_internal_with_method(terms1, terms2, sim_fn, groupwise, counter, ontology_terms)\n",
    "}\n",
    "\n",
    "\n",
    "/// Compute semantic similarity between two sets of GO terms.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// terms1 : list of str\n",
    "///   First list of GO term IDs.\n",
    "/// terms2 : list of str\n",
    "///   Second list of GO term IDs.\n",
    "/// term_similarity : str\n",
    "///   Name of the pairwise similarity method.\n",
    "/// groupwise : str\n",
    "///   Groupwise combination method. Options: \"bma\", \"max\", \"avg\", \"hausdorff\", \"simgic\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   Similarity score.\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (terms1, terms2, term_similarity=\"lin\", groupwise=\"bma\", counter=None))]\n",
    "pub fn termset_similarity(\n",
    "    terms1: Vec<String>,\n",
    "    terms2: Vec<String>,\n",
    "    term_similarity: &str,\n",
    "    groupwise: &str,\n",
    "    counter: Option<&TermCounter>,\n",
    ") -> PyResult<f64> {\n",
    "     let c = counter.ok_or_else(|| PyValueError::new_err(\"counter argument is required\"))?;\n",
    "     let terms_lock = get_terms_or_error()?;\n",
    "     termset_similarity_internal(&terms1, &terms2, term_similarity, groupwise, c, &terms_lock)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588c936",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "use pyo3::exceptions::PyValueError;\n",
    "use pyo3::prelude::*;\n",
    "use rayon::prelude::*;\n",
    "use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};\n",
    "\n",
    "use crate::go_loader::TermCounter;\n",
    "use crate::go_ontology::{get_gene2go_or_error, get_terms_or_error};\n",
    "\n",
    "use super::similarity::SimilarityMethod;\n",
    "use super::termset::{termset_similarity_internal, termset_similarity_internal_with_method};\n",
    "\n",
    "#[derive(Debug, Clone, Copy, PartialEq)]\n",
    "enum DistanceTransform {\n",
    "    OneMinus,\n",
    "    Reciprocal,\n",
    "    MaxMinus,\n",
    "}\n",
    "\n",
    "fn is_valid_groupwise(groupwise: &str) -> bool {\n",
    "    matches!(groupwise, \"bma\" | \"max\" | \"avg\" | \"hausdorff\" | \"simgic\")\n",
    "}\n",
    "\n",
    "fn is_normalized_similarity(similarity: &str, groupwise: &str) -> bool {\n",
    "    if groupwise == \"simgic\" {\n",
    "        return true;\n",
    "    }\n",
    "    matches!(\n",
    "        similarity.to_ascii_lowercase().as_str(),\n",
    "        \"lin\" | \"wang\" | \"simrel\" | \"topoicsim\"\n",
    "    )\n",
    "}\n",
    "\n",
    "fn resolve_distance_transform(\n",
    "    distance_transform: &str,\n",
    "    similarity: &str,\n",
    "    groupwise: &str,\n",
    ") -> PyResult<DistanceTransform> {\n",
    "    match distance_transform.to_ascii_lowercase().as_str() {\n",
    "        \"auto\" => {\n",
    "            if is_normalized_similarity(similarity, groupwise) {\n",
    "                Ok(DistanceTransform::OneMinus)\n",
    "            } else {\n",
    "                Ok(DistanceTransform::MaxMinus)\n",
    "            }\n",
    "        }\n",
    "        \"one_minus\" | \"one-minus\" | \"1-sim\" | \"1_minus\" => Ok(DistanceTransform::OneMinus),\n",
    "        \"reciprocal\" | \"inv\" | \"inverse\" => Ok(DistanceTransform::Reciprocal),\n",
    "        \"max_minus\" | \"max-minus\" | \"max\" => Ok(DistanceTransform::MaxMinus),\n",
    "        _ => Err(PyValueError::new_err(format!(\n",
    "            \"Unknown distance_transform '{}'. Options: auto, one_minus, reciprocal, max_minus\",\n",
    "            distance_transform\n",
    "        ))),\n",
    "    }\n",
    "}\n",
    "\n",
    "fn ontology_namespace(ontology: &str) -> PyResult<&'static str> {\n",
    "    match ontology.to_ascii_uppercase().as_str() {\n",
    "        \"BP\" => Ok(\"biological_process\"),\n",
    "        \"MF\" => Ok(\"molecular_function\"),\n",
    "        \"CC\" => Ok(\"cellular_component\"),\n",
    "        _ => Err(PyValueError::new_err(format!(\n",
    "            \"Invalid ontology '{}'. Must be 'BP', 'MF', or 'CC'\",\n",
    "            ontology\n",
    "        ))),\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Compute semantic similarity between genes.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// gene1 : str\n",
    "///   Gene symbol of the first gene.\n",
    "/// gene2 : str\n",
    "///   Gene symbol of the second gene.\n",
    "/// ontology : str\n",
    "///   Name of the subontology of GO to use: BP, MF or CC.\n",
    "/// similarity : str\n",
    "///   Name of the similarity method.\n",
    "/// groupwise : str\n",
    "///   Combination method to generate the similarities between genes. Options: \"bma\", \"max\", \"avg\", \"hausdorff\", \"simgic\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// float\n",
    "///   Similarity score.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If method or combine are unknown.\n",
    "#[pyfunction]\n",
    "pub fn compare_genes(\n",
    "    gene1: &str,\n",
    "    gene2: &str,\n",
    "    ontology: String,\n",
    "    similarity: &str,\n",
    "    groupwise: String,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<f64> {\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let gene2go = get_gene2go_or_error()?;\n",
    "    let g1_terms = gene2go.get(gene1).ok_or_else(|| {\n",
    "        pyo3::exceptions::PyValueError::new_err(format!(\"Gene '{}' not found in mapping\", gene1))\n",
    "    })?;\n",
    "    let g2_terms = gene2go.get(gene2).ok_or_else(|| {\n",
    "        pyo3::exceptions::PyValueError::new_err(format!(\"Gene '{}' not found in mapping\", gene2))\n",
    "    })?;\n",
    "    let ns = ontology_namespace(&ontology)?;\n",
    "    let f1: Vec<String> = g1_terms\n",
    "        .iter()\n",
    "        .filter(|id| terms.get(*id).map_or(false, |t| t.namespace.to_ascii_lowercase() == ns))\n",
    "        .cloned()\n",
    "        .collect();\n",
    "\n",
    "    let f2: Vec<String> = g2_terms\n",
    "        .iter()\n",
    "        .filter(|id| terms.get(*id).map_or(false, |t| t.namespace.to_ascii_lowercase() == ns))\n",
    "        .cloned()\n",
    "        .collect();\n",
    "\n",
    "    if f1.is_empty() || f2.is_empty() {\n",
    "        return Ok(0.0);\n",
    "    }\n",
    " \n",
    "    termset_similarity_internal(&f1, &f2, similarity, &groupwise, counter, &terms)\n",
    "}\n",
    "\n",
    "/// Compute semantic similarity between genes in batches.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// pairs : list of (str, str)\n",
    "///   List of pairs of genes to calculate the semantic similarity\n",
    "/// ontology : str\n",
    "///   Name of the subontology of GO to use: BP, MF or CC.\n",
    "/// similarity : str\n",
    "///   Name of the similarity method.\n",
    "/// groupwise : str\n",
    "///   Combination method to generate the similarities between genes. Options: \"bma\", \"max\", \"avg\", \"hausdorff\", \"simgic\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// list of float\n",
    "///   List of similarity scores.\n",
    "///\n",
    "/// Raises\n",
    "/// ------\n",
    "/// ValueError\n",
    "///   If method or combine are unknown.\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (pairs, ontology, similarity, groupwise, counter))]\n",
    "pub fn compare_gene_pairs_batch(\n",
    "    pairs: Vec<(String, String)>,\n",
    "    ontology: String,\n",
    "    similarity: &str,\n",
    "    groupwise: String,\n",
    "    counter: &TermCounter,\n",
    ") -> PyResult<Vec<f64>> {\n",
    "    let gene2go = get_gene2go_or_error()?;\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let ns = ontology_namespace(&ontology)?;\n",
    "    let sim_fn = if groupwise == \"simgic\" {\n",
    "        None\n",
    "    } else {\n",
    "        SimilarityMethod::from_str(similarity)\n",
    "    };\n",
    "\n",
    "    let mut unique_genes: HashSet<String> = HashSet::default();\n",
    "    for (g1, g2) in &pairs {\n",
    "        unique_genes.insert(g1.clone());\n",
    "        unique_genes.insert(g2.clone());\n",
    "    }\n",
    "\n",
    "    let gene_terms: HashMap<String, Vec<String>> = unique_genes\n",
    "        .into_iter()\n",
    "        .map(|gene| {\n",
    "            let filtered: Vec<String> = gene2go\n",
    "                .get(&gene)\n",
    "                .into_iter()\n",
    "                .flatten()\n",
    "                .filter(|go| {\n",
    "                    terms\n",
    "                        .get(go.as_str())\n",
    "                        .map_or(false, |t| t.namespace.eq_ignore_ascii_case(ns))\n",
    "                })\n",
    "                .cloned()\n",
    "                .collect();\n",
    "            (gene, filtered)\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    let empty: Vec<String> = Vec::new();\n",
    "    let scores: Vec<f64> = pairs\n",
    "        .par_iter()\n",
    "        .map(|(g1, g2)| {\n",
    "            let go1 = gene_terms.get(g1).unwrap_or(&empty);\n",
    "            let go2 = gene_terms.get(g2).unwrap_or(&empty);\n",
    "\n",
    "            if go1.is_empty() || go2.is_empty() {\n",
    "                return 0.0;\n",
    "            }\n",
    "\n",
    "            termset_similarity_internal_with_method(go1, go2, sim_fn, &groupwise, counter, &terms)\n",
    "                .unwrap_or(0.0)\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    Ok(scores)\n",
    "}\n",
    "\n",
    "/// Compute a gene-to-gene distance matrix using GO semantic similarity.\n",
    "///\n",
    "/// Arguments\n",
    "/// ---------\n",
    "/// genes : Optional[list[str]]\n",
    "///   List of genes to include. If None, uses all genes with annotations.\n",
    "/// ontology : str\n",
    "///   Name of the subontology of GO to use: BP, MF or CC.\n",
    "/// similarity : str\n",
    "///   Name of the similarity method.\n",
    "/// groupwise : str\n",
    "///   Combination method to generate the similarities between genes. Options: \"bma\", \"max\", \"avg\", \"hausdorff\", \"simgic\".\n",
    "/// counter : TermCounter\n",
    "///   Precomputed IC values.\n",
    "/// distance_transform : str\n",
    "///   How to convert similarity to distance. Options: \"auto\", \"one_minus\", \"reciprocal\", \"max_minus\".\n",
    "///\n",
    "/// Returns\n",
    "/// -------\n",
    "/// (list[str], list[list[float]])\n",
    "///   Tuple with the gene order and a square distance matrix.\n",
    "#[pyfunction]\n",
    "#[pyo3(signature = (genes=None, ontology=\"BP\", similarity=\"lin\", groupwise=\"bma\", counter=None, distance_transform=\"auto\"))]\n",
    "pub fn gene_distance_matrix(\n",
    "    genes: Option<Vec<String>>,\n",
    "    ontology: &str,\n",
    "    similarity: &str,\n",
    "    groupwise: &str,\n",
    "    counter: Option<&TermCounter>,\n",
    "    distance_transform: &str,\n",
    ") -> PyResult<(Vec<String>, Vec<Vec<f64>>)> {\n",
    "    let counter = counter.ok_or_else(|| PyValueError::new_err(\"counter argument is required\"))?;\n",
    "    if !is_valid_groupwise(groupwise) {\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"Unknown groupwise strategy: {}\",\n",
    "            groupwise\n",
    "        )));\n",
    "    }\n",
    "\n",
    "    let terms = get_terms_or_error()?;\n",
    "    let gene2go = get_gene2go_or_error()?;\n",
    "    let ns = ontology_namespace(ontology)?;\n",
    "\n",
    "    let gene_list = match genes {\n",
    "        Some(list) => list,\n",
    "        None => {\n",
    "            let mut all: Vec<String> = gene2go.keys().cloned().collect();\n",
    "            all.sort();\n",
    "            all\n",
    "        }\n",
    "    };\n",
    "\n",
    "    if gene_list.is_empty() {\n",
    "        return Ok((gene_list, Vec::new()));\n",
    "    }\n",
    "\n",
    "    let missing: Vec<String> = gene_list\n",
    "        .iter()\n",
    "        .filter(|g| !gene2go.contains_key(*g))\n",
    "        .cloned()\n",
    "        .collect();\n",
    "    if !missing.is_empty() {\n",
    "        return Err(PyValueError::new_err(format!(\n",
    "            \"Genes not found in mapping: {}\",\n",
    "            missing.join(\", \")\n",
    "        )));\n",
    "    }\n",
    "\n",
    "    let sim_fn = if groupwise == \"simgic\" {\n",
    "        None\n",
    "    } else {\n",
    "        Some(\n",
    "            SimilarityMethod::from_str(similarity)\n",
    "                .ok_or_else(|| PyValueError::new_err(format!(\"Unknown similarity method: {}\", similarity)))?\n",
    "        )\n",
    "    };\n",
    "\n",
    "    let gene_terms: Vec<Vec<String>> = gene_list\n",
    "        .par_iter()\n",
    "        .map(|gene| {\n",
    "            let terms_for_gene = gene2go.get(gene).unwrap();\n",
    "            terms_for_gene\n",
    "                .iter()\n",
    "                .filter(|go| {\n",
    "                    terms\n",
    "                        .get(go.as_str())\n",
    "                        .map_or(false, |t| t.namespace.eq_ignore_ascii_case(ns))\n",
    "                })\n",
    "                .cloned()\n",
    "                .collect()\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    let n = gene_list.len();\n",
    "    let mut matrix = vec![vec![0.0; n]; n];\n",
    "\n",
    "    // Parallelize over all (i, j) in the upper triangle (including diagonal) to improve\n",
    "    // load balancing across threads. This is especially important when each pairwise gene\n",
    "    // comparison is expensive (large term sets / slower similarity methods).\n",
    "    let pairs: Vec<(usize, usize)> = (0..n)\n",
    "        .flat_map(|i| (i..n).map(move |j| (i, j)))\n",
    "        .collect();\n",
    "\n",
    "    let sims: Vec<f64> = pairs\n",
    "        .par_iter()\n",
    "        .map(|(i, j)| {\n",
    "            termset_similarity_internal_with_method(\n",
    "                &gene_terms[*i],\n",
    "                &gene_terms[*j],\n",
    "                sim_fn,\n",
    "                groupwise,\n",
    "                counter,\n",
    "                &terms,\n",
    "            )\n",
    "            .unwrap_or(0.0)\n",
    "        })\n",
    "        .collect();\n",
    "\n",
    "    for ((i, j), sim) in pairs.into_iter().zip(sims.into_iter()) {\n",
    "        matrix[i][j] = sim;\n",
    "        matrix[j][i] = sim;\n",
    "    }\n",
    "\n",
    "    let transform = resolve_distance_transform(distance_transform, similarity, groupwise)?;\n",
    "    match transform {\n",
    "        DistanceTransform::MaxMinus => {\n",
    "            let mut max_sim = 0.0;\n",
    "            for row in &matrix {\n",
    "                for &v in row {\n",
    "                    if v > max_sim {\n",
    "                        max_sim = v;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            matrix.par_iter_mut().for_each(|row| {\n",
    "                for v in row.iter_mut() {\n",
    "                    let d = max_sim - *v;\n",
    "                    *v = if d < 0.0 { 0.0 } else { d };\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "        DistanceTransform::OneMinus => {\n",
    "            matrix.par_iter_mut().for_each(|row| {\n",
    "                for v in row.iter_mut() {\n",
    "                    let d = 1.0 - *v;\n",
    "                    *v = if d < 0.0 { 0.0 } else { d };\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "        DistanceTransform::Reciprocal => {\n",
    "            matrix.par_iter_mut().for_each(|row| {\n",
    "                for v in row.iter_mut() {\n",
    "                    *v = 1.0 / (1.0 + *v);\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for i in 0..n {\n",
    "        matrix[i][i] = 0.0;\n",
    "    }\n",
    "\n",
    "    Ok((gene_list, matrix))\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
